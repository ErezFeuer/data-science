[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "71253"
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#getting-to-know-pandas-and-jupyter-notebooks",
    "href": "tirgul/tirgul_1/tutorial_1.html#getting-to-know-pandas-and-jupyter-notebooks",
    "title": "1  Tutorial 1",
    "section": "1.1 Getting to Know Pandas and Jupyter Notebooks",
    "text": "1.1 Getting to Know Pandas and Jupyter Notebooks\nJupyter Notebooks are an open-source web application that can be used to create and share documents that contain live code, equations, visualizations and narrative text.\nPandas is a software library written for the Python programming language for data manipulation and analysis.\n\n1.1.1 In today’s tirgul we will learn:\n\nA bit about Jupyter Notebook\nHow to import data into Pandas and\nSome basic tools for manipulation."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#opening-a-jupyter-notebook",
    "href": "tirgul/tirgul_1/tutorial_1.html#opening-a-jupyter-notebook",
    "title": "1  Tutorial 1",
    "section": "1.2 Opening a Jupyter Notebook",
    "text": "1.2 Opening a Jupyter Notebook\nIf you’ve never used a Jupyter Notebook before…\n\nInstall Anaconda (https://docs.anaconda.com/anaconda/install/)\nOption 1: Open Anaconda Navigator and click on “Jupyter Notebook.” Option 2: Open terminal and type jupyter notebook followed by Enter.**\nNavigate to the folder (i.e., directory) that you want to be in.\nClick “New–&gt;Python 3 Notebook”\n\n**There are lots of other ways to do this. You can also use Visual Studio Code, or Binder… the choice is yours."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#about-jupyter-notebook",
    "href": "tirgul/tirgul_1/tutorial_1.html#about-jupyter-notebook",
    "title": "1  Tutorial 1",
    "section": "1.3 About Jupyter Notebook",
    "text": "1.3 About Jupyter Notebook\nJupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It’s a great tool for interactive data analysis, scientific computing, and educational purposes.\nIn Jupyter Notebook, you’ll see two main types of cells: code cells and markdown cells.\n\n1.3.1 Code cell\nCode cells are where you can write and execute Python (or other programming language) code. To create a new code cell, click on the + icon in the toolbar or use the Insert menu. To run a code cell, you can click the Run button in the toolbar, or press Shift + Enter on your keyboard. The code in the cell will be executed, and any output will be displayed below the cell.\n\n\n1.3.2 Markdown cell\nMarkdown cells are where you can write formatted text using Markdown syntax. Markdown is a lightweight markup language that allows you to add headings, lists, links, images, and more to your text. In fact, this is a markdown cell To create a new markdown cell, click on the + icon and select Markdown from the dropdown menu. To render the markdown as formatted text, you can also run the cell using Shift + Enter.\n\n\n1.3.3 Execution order\nThe order of appearance of cells in your notebook doesn’t necessarily determine the order in which they are executed. Instead, Jupyter Notebook keeps track of the order in which cells are run, and uses that order to ensure that all necessary code is executed before it’s used in later cells. You can see the execution order of cells by looking at the number to the left of the cell (e.g., [1]) - this indicates the order in which the cell was executed.\nIn summary, Jupyter Notebook is a powerful tool for interactive data analysis and scientific computing. Code cells allow you to write and execute Python code, while markdown cells allow you to write formatted text using Markdown syntax. You can run cells using the Run button or Shift + Enter, and Jupyter Notebook keeps track of the order in which cells are executed to ensure that your code runs correctly."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#short-markdown-example",
    "href": "tirgul/tirgul_1/tutorial_1.html#short-markdown-example",
    "title": "1  Tutorial 1",
    "section": "1.4 Short Markdown example",
    "text": "1.4 Short Markdown example\nTo create headers, you can use the “#” symbol followed by a space, and then type your header text. The number of “#” symbols determines the level of the header, with one “#” indicating the largest header and six “#” indicating the smallest header. For example:\n# This is a level 1 header\n\n## This is a level 2 header\n\n### This is a level 3 header\nTo go down a row and write bullet points, you can use the “*” symbol followed by a space, and then type your bullet text. For example:\n\nThis is a bullet point\nThis is another bullet point\n\n\n1.4.1 There are many more things you can do, this cheat sheet can be very helpful."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#download-the-required-dataset",
    "href": "tirgul/tirgul_1/tutorial_1.html#download-the-required-dataset",
    "title": "1  Tutorial 1",
    "section": "1.5 Download the required dataset",
    "text": "1.5 Download the required dataset\nWe are going to be using a dataset based on information from 60k OkCupid users. The dataset was collected by web scraping the OKCupid.com website on 2012/06/30, and includes profiles of people within a 25 mile radius of San Francisco, who were online in the previous year (after 06/30/2011), with at least one profile picture. The data includes age, sex, orientation as well as text data from open ended descriptions. You can download the dataset here: https://github.com/ErezFeuer/Introduction-to-Data-Science-Tutorials/blob/master/tirgul_1/okcupid_profiles.csv.\nHow to download from files from Github https://www.gitkraken.com/learn/git/github-download#how-to-downlaod-a-file-from-github"
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#getting-started-with-pandas",
    "href": "tirgul/tirgul_1/tutorial_1.html#getting-started-with-pandas",
    "title": "1  Tutorial 1",
    "section": "1.6 Getting started with Pandas",
    "text": "1.6 Getting started with Pandas\n\n# import the pandas library\nimport pandas as pd\n\n\n# import the dataset\nokcupid_df = pd.read_csv('okcupid_profiles.csv')\n\nYou just created a pandas DataFrame.\nWe can look at our data by using the .head() method.\nBy default, this shows the header (column names) and the first five rows.\nPassing an integer, n, to .head() returns that number of rows.\n\n# peak at the data\nokcupid_df.head(10)\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n0\n22\nsingle\nm\nstraight\na little extra\nstrictly anything\nsocially\nnever\nworking on college/university\nasian, white\n...\n-1\ntransportation\n2012-06-28-20-30\nsouth san francisco, california\ndoesn't have kids, but might want them\nlikes dogs and likes cats\nagnosticism and very serious about it\ngemini\nsometimes\nenglish\n\n\n1\n35\nsingle\nm\nstraight\naverage\nmostly other\noften\nsometimes\nworking on space camp\nwhite\n...\n80000\nhospitality / travel\n2012-06-29-21-41\noakland, california\ndoesn't have kids, but might want them\nlikes dogs and likes cats\nagnosticism but not too serious about it\ncancer\nno\nenglish (fluently), spanish (poorly), french (...\n\n\n2\n38\navailable\nm\nstraight\nthin\nanything\nsocially\nNaN\ngraduated from masters program\nNaN\n...\n-1\nNaN\n2012-06-27-09-10\nsan francisco, california\nNaN\nhas cats\nNaN\npisces but it doesn&rsquo;t matter\nno\nenglish, french, c++\n\n\n3\n23\nsingle\nm\nstraight\nthin\nvegetarian\nsocially\nNaN\nworking on college/university\nwhite\n...\n20000\nstudent\n2012-06-28-14-22\nberkeley, california\ndoesn't want kids\nlikes cats\nNaN\npisces\nno\nenglish, german (poorly)\n\n\n4\n29\nsingle\nm\nstraight\nathletic\nNaN\nsocially\nnever\ngraduated from college/university\nasian, black, other\n...\n-1\nartistic / musical / writer\n2012-06-27-21-26\nsan francisco, california\nNaN\nlikes dogs and likes cats\nNaN\naquarius\nno\nenglish\n\n\n5\n29\nsingle\nm\nstraight\naverage\nmostly anything\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n-1\ncomputer / hardware / software\n2012-06-29-19-18\nsan francisco, california\ndoesn't have kids, but might want them\nlikes cats\natheism\ntaurus\nno\nenglish (fluently), chinese (okay)\n\n\n6\n32\nsingle\nf\nstraight\nfit\nstrictly anything\nsocially\nnever\ngraduated from college/university\nwhite, other\n...\n-1\nNaN\n2012-06-25-20-45\nsan francisco, california\nNaN\nlikes dogs and likes cats\nNaN\nvirgo\nNaN\nenglish\n\n\n7\n31\nsingle\nf\nstraight\naverage\nmostly anything\nsocially\nnever\ngraduated from college/university\nwhite\n...\n-1\nartistic / musical / writer\n2012-06-29-12-30\nsan francisco, california\ndoesn't have kids, but wants them\nlikes dogs and likes cats\nchristianity\nsagittarius\nno\nenglish, spanish (okay)\n\n\n8\n24\nsingle\nf\nstraight\nNaN\nstrictly anything\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n-1\nNaN\n2012-06-29-23-39\nbelvedere tiburon, california\ndoesn't have kids\nlikes dogs and likes cats\nchristianity but not too serious about it\ngemini but it doesn&rsquo;t matter\nwhen drinking\nenglish\n\n\n9\n37\nsingle\nm\nstraight\nathletic\nmostly anything\nnot at all\nnever\nworking on two-year college\nwhite\n...\n-1\nstudent\n2012-06-28-21-08\nsan mateo, california\nNaN\nlikes dogs and likes cats\natheism and laughing about it\ncancer but it doesn&rsquo;t matter\nno\nenglish (fluently)\n\n\n\n\n10 rows × 21 columns\n\n\n\nAlternatively, to see the last n rows, use .tail().\n\nokcupid_df.tail(10)\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n9990\n33\nsingle\nm\nstraight\naverage\nmostly anything\nsocially\nnever\ngraduated from masters program\nasian\n...\n-1\nbanking / financial / real estate\n2012-06-30-08-51\nsan leandro, california\ndoesn't have kids\nlikes dogs\nNaN\nleo but it doesn&rsquo;t matter\nNaN\nenglish, chinese\n\n\n9991\n27\nsingle\nm\ngay\nathletic\nNaN\nsocially\nnever\ngraduated from college/university\nasian\n...\n-1\nstudent\n2011-10-13-01-58\nstanford, california\nNaN\nNaN\nNaN\nNaN\nno\nenglish (fluently), chinese (fluently)\n\n\n9992\n39\nsingle\nf\nstraight\nfit\nmostly anything\nsocially\nnever\ngraduated from masters program\nindian\n...\n-1\ncomputer / hardware / software\n2012-06-07-14-13\nsan francisco, california\ndoesn't have kids\nlikes dogs\nNaN\naquarius and it&rsquo;s fun to think about\nNaN\nenglish (fluently)\n\n\n9993\n38\nsingle\nf\nstraight\nathletic\nNaN\nsocially\nsometimes\ngraduated from college/university\nNaN\n...\n-1\ncomputer / hardware / software\n2012-06-30-01-51\noakland, california\ndoesn't have kids, but wants them\nNaN\nbuddhism\naries\nno\nenglish\n\n\n9994\n29\nsingle\nm\nstraight\naverage\nNaN\nsocially\nnever\ncollege/university\nNaN\n...\n-1\nNaN\n2012-01-09-21-35\noakland, california\nNaN\nNaN\nNaN\nNaN\nsometimes\nenglish (fluently), chinese (fluently)\n\n\n9995\n24\nsingle\nf\nstraight\ncurvy\nanything\nsocially\nnever\ngraduated from two-year college\nblack\n...\n20000\nother\n2012-01-16-23-10\nburlingame, california\nhas a kid\nlikes dogs and likes cats\nchristianity and very serious about it\nlibra but it doesn&rsquo;t matter\nwhen drinking\nenglish\n\n\n9996\n24\nsingle\nf\nstraight\nskinny\nNaN\nsocially\nnever\ngraduated from college/university\nasian\n...\n-1\nNaN\n2012-06-30-02-24\nsan francisco, california\ndoesn't have kids\nNaN\nNaN\nvirgo\nno\nenglish, chinese\n\n\n9997\n19\nseeing someone\nf\nbisexual\nathletic\nNaN\nrarely\nnever\nNaN\npacific islander, white, other\n...\n-1\nNaN\n2012-03-08-13-08\nmartinez, california\nNaN\nhas dogs and has cats\nNaN\naquarius but it doesn&rsquo;t matter\nsometimes\nenglish\n\n\n9998\n47\nsingle\nf\nstraight\na little extra\nNaN\nrarely\nnever\nNaN\nblack\n...\n50000\ntransportation\n2012-06-30-06-12\nemeryville, california\nNaN\nlikes dogs and likes cats\nother\nscorpio and it&rsquo;s fun to think about\nno\nenglish\n\n\n9999\n29\nsingle\nf\nstraight\nNaN\nmostly vegan\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n-1\nentertainment / media\n2012-06-03-23-30\nsan francisco, california\ndoesn't have kids, but might want them\nNaN\nNaN\nleo\nno\nenglish, spanish\n\n\n\n\n10 rows × 21 columns\n\n\n\n\n1.6.1 To find the number of rows, you can use the len() function. Alternatively, you can use the shape attribute.\n\n# get the length of the dataframe\nlen(okcupid_df)\n\n10000\n\n\n\n# get the size of the dataframe (rows, columns)\nokcupid_df.shape\n\n(10000, 21)\n\n\n\n\n1.6.2 What are all the columns in our dataset?\n\n# get columns\nokcupid_df.columns\n\nIndex(['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks',\n       'drugs', 'education', 'ethnicity', 'height', 'income', 'job',\n       'last_online', 'location', 'offspring', 'pets', 'religion', 'sign',\n       'smokes', 'speaks'],\n      dtype='object')\n\n\n\n# get column data types\nokcupid_df.dtypes\n\nage             int64\nstatus         object\nsex            object\norientation    object\nbody_type      object\ndiet           object\ndrinks         object\ndrugs          object\neducation      object\nethnicity      object\nheight          int64\nincome          int64\njob            object\nlast_online    object\nlocation       object\noffspring      object\npets           object\nreligion       object\nsign           object\nsmokes         object\nspeaks         object\ndtype: object\n\n\n\n\n1.6.3 Accessing a single column\n\n# access a single column\nokcupid_df[['height']]\n\n\n\n\n\n\n\n\nheight\n\n\n\n\n0\n75\n\n\n1\n70\n\n\n2\n68\n\n\n3\n71\n\n\n4\n66\n\n\n...\n...\n\n\n9995\n66\n\n\n9996\n59\n\n\n9997\n61\n\n\n9998\n64\n\n\n9999\n68\n\n\n\n\n10000 rows × 1 columns\n\n\n\n\n# let's try that a different way...\nokcupid_df['height']\n\n0       75\n1       70\n2       68\n3       71\n4       66\n        ..\n9995    66\n9996    59\n9997    61\n9998    64\n9999    68\nName: height, Length: 10000, dtype: int64\n\n\n\n# and another way...\nokcupid_df.height\n\n0       75\n1       70\n2       68\n3       71\n4       66\n        ..\n9995    66\n9996    59\n9997    61\n9998    64\n9999    68\nName: height, Length: 10000, dtype: int64\n\n\nIt is preferrable to use the bracket notation as a column name might inadvertently have the same name as a DataFrame (or Series) method. In addition, only bracket notation can be used to create a new column. If you try and use attribute access to create a new column, you’ll create a new attribute, not a new column.\nYou can either use a single bracket or a double bracket. The single bracket will output a pandas Series, while a double bracket will output a pandas DataFrame. A pandas Series is a single vector of data (e.g., a NumPy array) with “an associated array of data labels, called its index.” A DataFrame also has an index.\nIn our example, the indices are an array of sequential integers, which is the default. You can find them in the left-most position, without a column label. Indices need not be a sequence of integers. They can, for example, be dates or strings. Note that indices do not need to be unique."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#rename-index-and-slice",
    "href": "tirgul/tirgul_1/tutorial_1.html#rename-index-and-slice",
    "title": "1  Tutorial 1",
    "section": "1.7 Rename, Index, and Slice",
    "text": "1.7 Rename, Index, and Slice\nYou may have noticed that the height column data is in inches. Let’s rename the column so that the units are included in the name.\n\n# rename the height column\nokcupid_df.rename(columns={'height' : 'height_inches'}, inplace=True)\n\n\n# check to see that the change was implemented\nokcupid_df.columns\n\nIndex(['age', 'status', 'sex', 'orientation', 'body_type', 'diet', 'drinks',\n       'drugs', 'education', 'ethnicity', 'height_inches', 'income', 'job',\n       'last_online', 'location', 'offspring', 'pets', 'religion', 'sign',\n       'smokes', 'speaks'],\n      dtype='object')\n\n\nIndices, like column names, can be used to select data. Indices can be used to select particular rows. In fact, you can do something like .head() with slicing using the [] operator.\n\nokcupid_df[:5]\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n0\n22\nsingle\nm\nstraight\na little extra\nstrictly anything\nsocially\nnever\nworking on college/university\nasian, white\n...\n-1\ntransportation\n2012-06-28-20-30\nsouth san francisco, california\ndoesn't have kids, but might want them\nlikes dogs and likes cats\nagnosticism and very serious about it\ngemini\nsometimes\nenglish\n\n\n1\n35\nsingle\nm\nstraight\naverage\nmostly other\noften\nsometimes\nworking on space camp\nwhite\n...\n80000\nhospitality / travel\n2012-06-29-21-41\noakland, california\ndoesn't have kids, but might want them\nlikes dogs and likes cats\nagnosticism but not too serious about it\ncancer\nno\nenglish (fluently), spanish (poorly), french (...\n\n\n2\n38\navailable\nm\nstraight\nthin\nanything\nsocially\nNaN\ngraduated from masters program\nNaN\n...\n-1\nNaN\n2012-06-27-09-10\nsan francisco, california\nNaN\nhas cats\nNaN\npisces but it doesn&rsquo;t matter\nno\nenglish, french, c++\n\n\n3\n23\nsingle\nm\nstraight\nthin\nvegetarian\nsocially\nNaN\nworking on college/university\nwhite\n...\n20000\nstudent\n2012-06-28-14-22\nberkeley, california\ndoesn't want kids\nlikes cats\nNaN\npisces\nno\nenglish, german (poorly)\n\n\n4\n29\nsingle\nm\nstraight\nathletic\nNaN\nsocially\nnever\ngraduated from college/university\nasian, black, other\n...\n-1\nartistic / musical / writer\n2012-06-27-21-26\nsan francisco, california\nNaN\nlikes dogs and likes cats\nNaN\naquarius\nno\nenglish\n\n\n\n\n5 rows × 21 columns\n\n\n\nBefore we continue, let’s spend some more time looking at a few useful ways to index data—that is, select rows. .loc primarily works with string labels. It accepts a single label, a list (or array) of labels, or a slice of labels (e.g., ‘a’ : ‘f’).\n\n# another way to get a single column...\nokcupid_df.loc[:, 'sex']\n\n0       m\n1       m\n2       m\n3       m\n4       m\n       ..\n9995    f\n9996    f\n9997    f\n9998    f\n9999    f\nName: sex, Length: 10000, dtype: object\n\n\n\n# we can also index based on rows\nokcupid_df[2:4]\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n2\n38\navailable\nm\nstraight\nthin\nanything\nsocially\nNaN\ngraduated from masters program\nNaN\n...\n-1\nNaN\n2012-06-27-09-10\nsan francisco, california\nNaN\nhas cats\nNaN\npisces but it doesn&rsquo;t matter\nno\nenglish, french, c++\n\n\n3\n23\nsingle\nm\nstraight\nthin\nvegetarian\nsocially\nNaN\nworking on college/university\nwhite\n...\n20000\nstudent\n2012-06-28-14-22\nberkeley, california\ndoesn't want kids\nlikes cats\nNaN\npisces\nno\nenglish, german (poorly)\n\n\n\n\n2 rows × 21 columns\n\n\n\nThe difference is that the former returns a Series because we selected a single lable, while the latter returns a DataFrame because we selected a range of positions.\nAnother indexing option, .iloc, primarily works with integer positions. To select specific rows, we can do the following.\n\nokcupid_df.iloc[[1, 5, 6, 9]]\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n1\n35\nsingle\nm\nstraight\naverage\nmostly other\noften\nsometimes\nworking on space camp\nwhite\n...\n80000\nhospitality / travel\n2012-06-29-21-41\noakland, california\ndoesn't have kids, but might want them\nlikes dogs and likes cats\nagnosticism but not too serious about it\ncancer\nno\nenglish (fluently), spanish (poorly), french (...\n\n\n5\n29\nsingle\nm\nstraight\naverage\nmostly anything\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n-1\ncomputer / hardware / software\n2012-06-29-19-18\nsan francisco, california\ndoesn't have kids, but might want them\nlikes cats\natheism\ntaurus\nno\nenglish (fluently), chinese (okay)\n\n\n6\n32\nsingle\nf\nstraight\nfit\nstrictly anything\nsocially\nnever\ngraduated from college/university\nwhite, other\n...\n-1\nNaN\n2012-06-25-20-45\nsan francisco, california\nNaN\nlikes dogs and likes cats\nNaN\nvirgo\nNaN\nenglish\n\n\n9\n37\nsingle\nm\nstraight\nathletic\nmostly anything\nnot at all\nnever\nworking on two-year college\nwhite\n...\n-1\nstudent\n2012-06-28-21-08\nsan mateo, california\nNaN\nlikes dogs and likes cats\natheism and laughing about it\ncancer but it doesn&rsquo;t matter\nno\nenglish (fluently)\n\n\n\n\n4 rows × 21 columns\n\n\n\nWe can select a range of rows and specify the step value.\n\nokcupid_df.iloc[25:50:5]\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nincome\njob\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\n\n\n\n\n25\n28\nsingle\nm\nstraight\nfit\nanything\nrarely\nnever\ngraduated from college/university\nasian, white\n...\n-1\nmedicine / health\n2012-06-26-01-27\natherton, california\ndoesn't have kids, but wants them\nhas dogs\nNaN\ngemini and it&rsquo;s fun to think about\nno\nenglish\n\n\n30\n27\nsingle\nf\nstraight\naverage\nanything\nsocially\nNaN\nworking on college/university\nwhite\n...\n-1\nother\n2011-11-10-13-15\nsan francisco, california\nNaN\nNaN\nagnosticism\ngemini\ntrying to quit\nenglish, spanish (poorly)\n\n\n35\n26\nsingle\nm\nstraight\nathletic\nNaN\nNaN\nnever\nNaN\nwhite\n...\n-1\nbanking / financial / real estate\n2012-06-27-00-32\nsan francisco, california\nNaN\nNaN\nNaN\nNaN\nNaN\nenglish, russian\n\n\n40\n30\nsingle\nm\nstraight\naverage\nNaN\noften\nnever\ngraduated from masters program\nNaN\n...\n-1\ncomputer / hardware / software\n2012-06-29-22-56\nmenlo park, california\ndoesn't have kids\nlikes cats\nagnosticism\nNaN\nno\nenglish (fluently), dutch (fluently), lisp (fl...\n\n\n45\n27\nsingle\nm\nstraight\naverage\nmostly anything\nsocially\nnever\ncollege/university\npacific islander\n...\n-1\neducation / academia\n2012-06-28-18-09\noakland, california\ndoesn't have kids\nNaN\ncatholicism and somewhat serious about it\naquarius\nNaN\nenglish (fluently), tagalog (fluently)\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n1.7.1 We can also create a new column\nHere we create a new column with height in cm\n\nokcupid_df['height_cm'] = okcupid_df['height_inches']*2.54\nokcupid_df['height_cm']\n\n0       190.50\n1       177.80\n2       172.72\n3       180.34\n4       167.64\n         ...  \n9995    167.64\n9996    149.86\n9997    154.94\n9998    162.56\n9999    172.72\nName: height_cm, Length: 10000, dtype: float64\n\n\n\n1.7.1.1 Another example\n\nokcupid_df['speaks']\n\n0                                                 english\n1       english (fluently), spanish (poorly), french (...\n2                                    english, french, c++\n3                                english, german (poorly)\n4                                                 english\n                              ...                        \n9995                                              english\n9996                                     english, chinese\n9997                                              english\n9998                                              english\n9999                                     english, spanish\nName: speaks, Length: 10000, dtype: object\n\n\nNow we’ll create a column that has only the first language the person speaks.\n\nokcupid_df['speaks first'] = okcupid_df['speaks'].dropna().apply(lambda x: x.split(',')[0])\nokcupid_df['speaks first']\n\n0                  english\n1       english (fluently)\n2                  english\n3                  english\n4                  english\n               ...        \n9995               english\n9996               english\n9997               english\n9998               english\n9999               english\nName: speaks first, Length: 10000, dtype: object\n\n\n\n\n\n1.7.2 Slicing by condition\nLets say we want a df with only the females.\n\nf_okcupid_df = okcupid_df.loc[okcupid_df['sex'] == 'f']\nf_okcupid_df\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\nheight_cm\nspeaks first\n\n\n\n\n6\n32\nsingle\nf\nstraight\nfit\nstrictly anything\nsocially\nnever\ngraduated from college/university\nwhite, other\n...\n2012-06-25-20-45\nsan francisco, california\nNaN\nlikes dogs and likes cats\nNaN\nvirgo\nNaN\nenglish\n165.10\nenglish\n\n\n7\n31\nsingle\nf\nstraight\naverage\nmostly anything\nsocially\nnever\ngraduated from college/university\nwhite\n...\n2012-06-29-12-30\nsan francisco, california\ndoesn't have kids, but wants them\nlikes dogs and likes cats\nchristianity\nsagittarius\nno\nenglish, spanish (okay)\n165.10\nenglish\n\n\n8\n24\nsingle\nf\nstraight\nNaN\nstrictly anything\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n2012-06-29-23-39\nbelvedere tiburon, california\ndoesn't have kids\nlikes dogs and likes cats\nchristianity but not too serious about it\ngemini but it doesn&rsquo;t matter\nwhen drinking\nenglish\n170.18\nenglish\n\n\n13\n30\nsingle\nf\nstraight\nskinny\nmostly anything\nsocially\nnever\ngraduated from high school\nwhite\n...\n2012-06-13-16-06\nsan francisco, california\nNaN\nhas dogs and likes cats\nchristianity but not too serious about it\nNaN\nno\nenglish\n167.64\nenglish\n\n\n14\n29\nsingle\nf\nstraight\nthin\nmostly anything\nsocially\nnever\nworking on college/university\nhispanic / latin, white\n...\n2012-06-29-08-55\nsan leandro, california\ndoesn't have kids, but wants them\nlikes dogs and has cats\ncatholicism\ntaurus\nno\nenglish\n157.48\nenglish\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n24\nsingle\nf\nstraight\ncurvy\nanything\nsocially\nnever\ngraduated from two-year college\nblack\n...\n2012-01-16-23-10\nburlingame, california\nhas a kid\nlikes dogs and likes cats\nchristianity and very serious about it\nlibra but it doesn&rsquo;t matter\nwhen drinking\nenglish\n167.64\nenglish\n\n\n9996\n24\nsingle\nf\nstraight\nskinny\nNaN\nsocially\nnever\ngraduated from college/university\nasian\n...\n2012-06-30-02-24\nsan francisco, california\ndoesn't have kids\nNaN\nNaN\nvirgo\nno\nenglish, chinese\n149.86\nenglish\n\n\n9997\n19\nseeing someone\nf\nbisexual\nathletic\nNaN\nrarely\nnever\nNaN\npacific islander, white, other\n...\n2012-03-08-13-08\nmartinez, california\nNaN\nhas dogs and has cats\nNaN\naquarius but it doesn&rsquo;t matter\nsometimes\nenglish\n154.94\nenglish\n\n\n9998\n47\nsingle\nf\nstraight\na little extra\nNaN\nrarely\nnever\nNaN\nblack\n...\n2012-06-30-06-12\nemeryville, california\nNaN\nlikes dogs and likes cats\nother\nscorpio and it&rsquo;s fun to think about\nno\nenglish\n162.56\nenglish\n\n\n9999\n29\nsingle\nf\nstraight\nNaN\nmostly vegan\nsocially\nNaN\ngraduated from college/university\nwhite\n...\n2012-06-03-23-30\nsan francisco, california\ndoesn't have kids, but might want them\nNaN\nNaN\nleo\nno\nenglish, spanish\n172.72\nenglish\n\n\n\n\n4056 rows × 23 columns"
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#summary-statistics",
    "href": "tirgul/tirgul_1/tutorial_1.html#summary-statistics",
    "title": "1  Tutorial 1",
    "section": "1.8 Summary statistics",
    "text": "1.8 Summary statistics\n\n1.8.1 A useful method that generates various summary statistics is .describe().\n\n# get summary statistics\nokcupid_df.describe()\n\n\n\n\n\n\n\n\nage\nheight_inches\nincome\nheight_cm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n32.073500\n68.331200\n19307.189900\n173.561248\n\n\nstd\n9.444025\n3.908482\n93842.703841\n9.927545\n\n\nmin\n18.000000\n36.000000\n-1.000000\n91.440000\n\n\n25%\n25.000000\n66.000000\n-1.000000\n167.640000\n\n\n50%\n30.000000\n68.000000\n-1.000000\n172.720000\n\n\n75%\n36.000000\n71.000000\n-1.000000\n180.340000\n\n\nmax\n110.000000\n95.000000\n1000000.000000\n241.300000\n\n\n\n\n\n\n\nThe summary statistics are based on non-missing values and count reflects that. The values depend on what it’s called on. If the DataFrame includes both numeric and object (e.g., strings) dtypes, it will default to summarizing the numeric data. If .describe() is called on strings, for example, it will return the count, number of unique values, and the most frequent value along with its count.\n\n# describe can be used on a single column\nokcupid_df['income'].describe()\n\ncount      10000.000000\nmean       19307.189900\nstd        93842.703841\nmin           -1.000000\n25%           -1.000000\n50%           -1.000000\n75%           -1.000000\nmax      1000000.000000\nName: income, dtype: float64\n\n\n\n# and on non-numerical data  \nokcupid_df['sex'].describe()\n\ncount     10000\nunique        2\ntop           m\nfreq       5944\nName: sex, dtype: object\n\n\n\n\n1.8.2 Unique values\nWe can find out how the unique values in a column\n\nokcupid_df['drinks'].unique()\n\narray(['socially', 'often', 'not at all', 'rarely', nan, 'very often',\n       'desperately'], dtype=object)"
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#manipulating-data",
    "href": "tirgul/tirgul_1/tutorial_1.html#manipulating-data",
    "title": "1  Tutorial 1",
    "section": "1.9 Manipulating Data",
    "text": "1.9 Manipulating Data\n\n1.9.1 Let’s seperate the data based on sex\n\n# male users\nmales=okcupid_df.loc[okcupid_df[\"sex\"]==\"m\"] \nn_males = len(males)\n\n# female users\nfemales=okcupid_df.loc[okcupid_df[\"sex\"]==\"f\"] \nn_females = len(females)\n\nn_population = len(okcupid_df)\n\n# how many users are male and how many are female?\nprint(f\"{n_males} males ({n_males/n_population:.1%}), {n_females} females ({n_females/n_population})\")\n\n5944 males (59.4%), 4056 females (0.4056)\n\n\nNote that above I set the male percentage for 1 decimal point with :.1 and for the female I didn’t\n\n\n1.9.2 Let’s examine the age distribution\n\n# summary statistics on the 'age' column\nokcupid_df['age'].describe()\n\ncount    10000.000000\nmean        32.073500\nstd          9.444025\nmin         18.000000\n25%         25.000000\n50%         30.000000\n75%         36.000000\nmax        110.000000\nName: age, dtype: float64\n\n\n\n# how many users are older than 80?\n(okcupid_df[\"age\"]&gt;80).sum()\n\n1\n\n\n\n# can also be written this way\nsum(okcupid_df['age']&gt;80)\n\n1\n\n\n\n\n1.9.3 Who are the age outliers?\n\nokcupid_df[okcupid_df[\"age\"]&gt;80]\n\n\n\n\n\n\n\n\nage\nstatus\nsex\norientation\nbody_type\ndiet\ndrinks\ndrugs\neducation\nethnicity\n...\nlast_online\nlocation\noffspring\npets\nreligion\nsign\nsmokes\nspeaks\nheight_cm\nspeaks first\n\n\n\n\n2512\n110\nsingle\nf\nstraight\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2012-06-27-22-16\ndaly city, california\nNaN\nNaN\nNaN\nNaN\nNaN\nenglish\n170.18\nenglish\n\n\n\n\n1 rows × 23 columns\n\n\n\nLet’s assume the 110-year-old lady and the athletic 109-year-old gentleman (who’s working on a masters program) are outliers: we get rid of them so the following plots look better. They didn’t say much else about themselves, anyway.\n\n# eliminate users with age &gt;= 80\nokcupid_df=okcupid_df[okcupid_df[\"age\"]&lt;=80] \n\nIs the age distribution for men different than that for women?\n\nprint(\"Mean and median age for males:   {:.2f}, {:.2f}\".format(males[\"age\"].mean(),males[\"age\"].median()))\nprint(\"Mean and median age for females: {:.2f}, {:.2f}\".format(females[\"age\"].mean(),females[\"age\"].median()))\n\nMean and median age for males:   31.75, 30.00\nMean and median age for females: 32.55, 30.00\n\n\n\n\n1.9.4 We can also plot the data (don’t worry if you don’t understand the code)\n\n# add plotting libraries to notebook\n%matplotlib inline\n%config InlineBackend.figure_format='svg'\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nsns.set_style(\"ticks\")\nsns.set_context(context=\"notebook\",font_scale=1)\n\n\n# draw age histograms for male and female users\nfig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(10,3),sharey=True,sharex=True)\nsns.histplot(males[\"age\"], ax=ax1,\n             bins=range(okcupid_df[\"age\"].min(),okcupid_df[\"age\"].max()),\n             kde=False,\n             color=\"b\")\nax1.set_title(\"Age distribution for males\")\nsns.histplot(females[\"age\"], ax=ax2,\n             bins=range(okcupid_df[\"age\"].min(),okcupid_df[\"age\"].max()),\n             kde=False,\n             color=\"r\")\nax2.set_title(\"Age distribution for females\")\nax1.set_ylabel(\"Number of users in age group\")\nfor ax in (ax1,ax2):\n    sns.despine(ax=ax)\nfig.tight_layout()\n\n\n\n\n\n\n1.9.5 Now let’s study height distribution and compare with official data from the US Centers of Disease Control and Prevention\n\nfig,(ax,ax2) = plt.subplots(nrows=2,sharex=True,figsize=(6,6),gridspec_kw={'height_ratios':[2,1]})\n\n# Plot histograms of height\nbins=range(55,80)\nsns.histplot(males[\"height_inches\"].dropna(), ax=ax,\n             bins=bins,\n             kde=False,\n             color=\"b\",\n             label=\"males\")\nsns.histplot(females[\"height_inches\"].dropna(), ax=ax,\n             bins=bins,\n             kde=False,\n             color=\"r\",\n             label=\"females\")\nax.legend(loc=\"upper left\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of users with given height\")\nax.set_title(\"height distribution of male and female users\");\n\n# Make aligned boxplots\nsns.boxplot(data=okcupid_df,y=\"sex\",x=\"height_inches\",orient=\"h\",ax=ax2,palette={\"m\":\"b\",\"f\":\"r\"})\nplt.setp(ax2.artists, alpha=.5)\nax2.set_xlim([min(bins),max(bins)])\nax2.set_xlabel(\"Self-reported height [inches]\")\n\nsns.despine(ax=ax)\nfig.tight_layout()\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_35920/2199488932.py:21: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=okcupid_df,y=\"sex\",x=\"height_inches\",orient=\"h\",ax=ax2,palette={\"m\":\"b\",\"f\":\"r\"})\n\n\n\n\n\nMales are obviously taller than females, and the two distributions make sense.\n\n\n1.9.6 How do OkCupid user’s heights compare to the general population?\nHow does this compare with general population data? Are OkCupid users maybe cheating and overreporting their height?\nThe CDC publishes growth charts, which contain height data for the general US population. The dataset reports statistics (3rd, 5th, 10th, 25th, 50th, 75th, 90th, 95th, 97th percentiles) for stature for different ages from 2 to 20 years. This (and more) data is plotted by the CDC in these beautiful charts (https://www.cdc.gov/growthcharts/data/set2clinical/set2color.pdf).\n\n# Import a CSV file for growth chart data\ncdc=pd.read_csv(\"https://www.cdc.gov/growthcharts/data/zscore/statage.csv\")\n\n# print first ten rows of data\ncdc.head(10)\n\n\n\n\n\n\n\n\nSex\nAgemos\nL\nM\nS\nP3\nP5\nP10\nP25\nP50\nP75\nP90\nP95\nP97\n\n\n\n\n0\n1\n24.0\n0.941524\n86.452201\n0.040322\n79.910844\n80.729773\n81.991714\n84.102892\n86.452201\n88.805249\n90.926191\n92.196879\n93.022654\n\n\n1\n1\n24.5\n1.007208\n86.861609\n0.040396\n80.260371\n81.088685\n82.364010\n84.494706\n86.861609\n89.228048\n91.357530\n92.631767\n93.459230\n\n\n2\n1\n25.5\n0.837251\n87.652473\n0.040578\n81.005294\n81.834452\n83.113871\n85.258877\n87.652473\n90.056755\n92.229661\n93.534066\n94.382780\n\n\n3\n1\n26.5\n0.681493\n88.423264\n0.040723\n81.734157\n82.564061\n83.847162\n86.005173\n88.423264\n90.862604\n93.076082\n94.408849\n95.277617\n\n\n4\n1\n27.5\n0.538780\n89.175492\n0.040833\n82.448456\n83.278986\n84.565344\n86.735069\n89.175492\n91.647114\n93.898271\n95.257541\n96.145118\n\n\n5\n1\n28.5\n0.407697\n89.910409\n0.040909\n83.149450\n83.980453\n85.269620\n87.449772\n89.910409\n92.411590\n94.697570\n96.081488\n96.986625\n\n\n6\n1\n29.5\n0.286762\n90.629078\n0.040952\n83.838194\n84.669484\n85.960983\n88.150284\n90.629078\n93.157190\n95.475221\n96.881980\n97.803453\n\n\n7\n1\n30.5\n0.174489\n91.332424\n0.040965\n84.515583\n85.346943\n86.640272\n88.837454\n91.332424\n93.884956\n96.232394\n97.660267\n98.596905\n\n\n8\n1\n31.5\n0.069445\n92.021272\n0.040950\n85.182380\n86.013566\n87.308201\n89.512019\n92.021272\n94.595854\n96.970215\n98.417579\n99.368283\n\n\n9\n1\n32.5\n-0.029721\n92.696379\n0.040909\n85.839250\n86.669993\n87.965401\n90.174637\n92.696379\n95.290799\n97.689781\n99.155138\n100.118893\n\n\n\n\n\n\n\n\n# Adjust the data to fit our format\ncdc[\"Age\"]=cdc[\"Agemos\"]/12 # convert age in months to age in fractional years\ncdc[\"Sex\"]=cdc[\"Sex\"].replace({1:\"male\",2:\"female\"}) # align to our convention\n\n# group the data into percentiles\npercentiles=[3,5,10,25,50,75,90,95,97]\npercentile_columns=[\"P\"+str(p) for p in percentiles] # names of percentile columns\ncdc[percentile_columns]=cdc[percentile_columns]*0.393701 # convert percentile columns from centimeters to inches (ugh)\ncdc20=cdc[cdc[\"Age\"]==20].set_index(\"Sex\") # Select the two rows corresponding to 20-year-olds (males and females)\n\nprint(\"Height Percentiles for 20-year-old US population [inches]\")\ncdc20[percentile_columns]\n\nHeight Percentiles for 20-year-old US population [inches]\n\n\n\n\n\n\n\n\n\nP3\nP5\nP10\nP25\nP50\nP75\nP90\nP95\nP97\n\n\nSex\n\n\n\n\n\n\n\n\n\n\n\n\n\nmale\n64.304496\n64.975961\n66.007627\n67.725520\n69.625720\n71.517283\n73.212615\n74.224061\n74.879691\n\n\nfemale\n59.492618\n60.098742\n61.030770\n62.584736\n64.306433\n66.023163\n67.564157\n68.484561\n69.081584\n\n\n\n\n\n\n\n\n\n1.9.7 Let’s compare the stats for reported heights of the OkCupid 20-year-olds to the CDC stats for 20-year-olds.\n\nmheights=males.loc[males[\"age\"]==20,\"height_inches\"] # heights of 20-year-old males\nfheights=females.loc[females[\"age\"]==20,\"height_inches\"] # heights of 20-year-old females\n\n# To smooth the computation of percentiles, jitter height data by adding\n# uniformly distributed noise in the range [-0.5,+0.5]\nmheightsj=mheights+np.random.uniform(low=-0.5,high=+0.5,size=(len(mheights),))\nfheightsj=fheights+np.random.uniform(low=-0.5,high=+0.5,size=(len(fheights),))\n\n# For each of the available percentiles in CDC data, compute the corresponding percentile from our 20-year-old users\nstats=[]\nfor percentile,percentile_column in zip(percentiles,percentile_columns):\n    stats.append({\"sex\":\"male\",\n                  \"percentile\":percentile,\n                  \"CDC\":cdc20.loc[\"male\",percentile_column],\n                  \"OkCupid\":mheightsj.quantile(percentile/100)})\n    stats.append({\"sex\":\"female\",\n                  \"percentile\":percentile,\n                  \"CDC\":cdc20.loc[\"female\",percentile_column],\n                  \"OkCupid\":fheightsj.quantile(percentile/100)})\nstats=pd.DataFrame(stats).set_index([\"sex\",\"percentile\"]).sort_index()\n\n# For each percentile, compute the gap between users and CDC\nstats[\"gap\"]=stats[\"OkCupid\"]-stats[\"CDC\"]\n\n\n# plot data\nfig,(ax1,ax2)=plt.subplots(ncols=2,sharex=True,figsize=(10,4))\nstats.loc[\"male\"][[\"OkCupid\",\"CDC\"]].plot.bar(ax=ax1,color=[[0.5,0.5,1],\"k\"],alpha=1,width=0.8,rot=0)\nstats.loc[\"female\"][[\"OkCupid\",\"CDC\"]].plot.bar(ax=ax2,color=[[1,0.5,0.5],\"k\"],alpha=1,width=0.8,rot=0)\nax1.set_ylim([64,77])\nax2.set_ylim([58,71])\nax1.set_ylabel(\"Height [inches]\")\nax2.set_ylabel(\"Height [inches]\")\nax1.set_title(\"Height percentiles in 20y-old male users vs CDC data\")\nax2.set_title(\"Height percentiles in 20y-old female users vs CDC data\")\nfor ax in (ax1,ax2):\n    sns.despine(ax=ax)\nfig.tight_layout()\n\n\n\n\nThe height statistics of our user population matches remarkably well with CDC data. It looks like the OkCupid users, males and females alike, may be slightly over-reporting their height (still, much less than one could expect), but there could be many other reasonable explanations for this gap. For example, the San-Francisco population may be taller than the general US population. This might be a starting point to further investigate the issue."
  },
  {
    "objectID": "tirgul/tirgul_1/tutorial_1.html#more-pandas-resources",
    "href": "tirgul/tirgul_1/tutorial_1.html#more-pandas-resources",
    "title": "1  Tutorial 1",
    "section": "1.10 More Pandas Resources",
    "text": "1.10 More Pandas Resources\n\nLectures on Pandas from UC Berekely https://ds100.org/fa20/lecture/lec05/\nGetting started with Pandas https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html#getting-started\n10 minute guide to Pandas https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\nThe Pandas Cookbook. This provides a nice overview of some of the basic Pandas functions. However, it is slightly out of date. https://nbviewer.jupyter.org/github/jvns/pandas-cookbook/tree/master/cookbook/\nLearn Pandas. A set of lessons providing an overview of the Pandas library. https://bitbucket.org/hrojas/learn-pandas/src/master/ Python for Data Science Another set of notebook demonstrating Pandas functionality."
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#topics",
    "href": "tirgul/tirgul_2/tirgul_2.html#topics",
    "title": "2  Tutorial 2",
    "section": "2.1 Topics",
    "text": "2.1 Topics\n\nSummary statistics\nQuantiles\nHistograms\nEncoding categorical varaibles"
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#important-python-packages",
    "href": "tirgul/tirgul_2/tirgul_2.html#important-python-packages",
    "title": "2  Tutorial 2",
    "section": "2.2 Important Python Packages",
    "text": "2.2 Important Python Packages\n\nPandas\nSeaborn\nMatplotlib\n\n\n# import necessary packages\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#todays-datasets",
    "href": "tirgul/tirgul_2/tirgul_2.html#todays-datasets",
    "title": "2  Tutorial 2",
    "section": "2.3 Today’s datasets",
    "text": "2.3 Today’s datasets\n\nCity Temperatures – Daily temperature for different international cities (download .csv here)\n\n\n# read CSV\ntemp_df = pd.read_csv('city_temp.csv')\n\n\n# examine the data -- in visual studio code we can also do this another way\ntemp_df\n\n\n\n\n\n\n\n\nCountry\nCity\nMonth\nDay\nYear\nAvgTemperature\n\n\n\n\n0\nMalawi\nLilongwe\n1\n1\n1995\n69.5\n\n\n1\nMalawi\nLilongwe\n1\n2\n1995\n69.5\n\n\n2\nMalawi\nLilongwe\n1\n3\n1995\n67.5\n\n\n3\nMalawi\nLilongwe\n1\n4\n1995\n68.5\n\n\n4\nMalawi\nLilongwe\n1\n5\n1995\n66.7\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n47392\nUS\nRochester\n5\n9\n2020\n33.9\n\n\n47393\nUS\nRochester\n5\n10\n2020\n41.4\n\n\n47394\nUS\nRochester\n5\n11\n2020\n40.7\n\n\n47395\nUS\nRochester\n5\n12\n2020\n38.9\n\n\n47396\nUS\nRochester\n5\n13\n2020\n34.0\n\n\n\n\n47397 rows × 6 columns\n\n\n\n\n# which cities do we have data for\ntemp_df['City'].unique()\n\narray(['Lilongwe', 'Capetown', 'Tel Aviv', 'Amman', 'Beirut', 'Rochester'],\n      dtype=object)\n\n\n\n# isolate data from a single city (e.g., Tel Aviv)\ncity = 'Tel Aviv'\nTA_temp = temp_df.loc[temp_df['City'] == city]\nTA_temp\n\n\n\n\n\n\n\n\nCountry\nCity\nMonth\nDay\nYear\nAvgTemperature\n\n\n\n\n14959\nIsrael\nTel Aviv\n1\n1\n1995\n57.3\n\n\n14960\nIsrael\nTel Aviv\n1\n2\n1995\n56.1\n\n\n14961\nIsrael\nTel Aviv\n1\n3\n1995\n55.9\n\n\n14962\nIsrael\nTel Aviv\n1\n4\n1995\n56.9\n\n\n14963\nIsrael\nTel Aviv\n1\n5\n1995\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n19595\nIsrael\nTel Aviv\n9\n11\n2007\n79.5\n\n\n19596\nIsrael\nTel Aviv\n9\n12\n2007\n79.7\n\n\n19597\nIsrael\nTel Aviv\n9\n13\n2007\n79.7\n\n\n19598\nIsrael\nTel Aviv\n9\n14\n2007\n79.6\n\n\n19599\nIsrael\nTel Aviv\n9\n15\n2007\n80.0\n\n\n\n\n4641 rows × 6 columns\n\n\n\n\n# get summary statistics for a single city\nTA_temp['AvgTemperature'].describe()\n\ncount    4641.000000\nmean       54.020448\nstd        50.624184\nmin       -99.000000\n25%        59.400000\n50%        68.700000\n75%        78.600000\nmax        88.500000\nName: AvgTemperature, dtype: float64\n\n\n\nTA_temp = TA_temp.loc[TA_temp['AvgTemperature']&gt;(-50)]\n\n\n# convert to Celsius\nTA_temp['AvgTemp_C'] = (TA_temp['AvgTemperature'] - 32)*(5/9)\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_82655/3623248864.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  TA_temp['AvgTemp_C'] = (TA_temp['AvgTemperature'] - 32)*(5/9)\n\n\n\n# get summary stasitics in Celsius\nTA_temp['AvgTemp_C'].describe()\n\ncount    4196.000000\nmean       21.249325\nstd         5.193370\nmin         7.277778\n25%        16.555556\n50%        21.611111\n75%        26.277778\nmax        31.388889\nName: AvgTemp_C, dtype: float64\n\n\n\n# get the mean for the city you chose\nmean_temp = TA_temp['AvgTemp_C'].mean() \nprint(f\"The mean temperature in {city} is: {mean_temp:.2f} degrees Celcius\")\n\nThe mean temperature in Tel Aviv is: 21.25 degrees Celcius\n\n\n\n# get the median temperature for the city you chose\nmedian_temp = TA_temp['AvgTemp_C'].median() \nprint(f\"The median temperature in {city} is: {median_temp:.2f} degrees Celcius\")\n\nThe median temperature in Tel Aviv is: 21.61 degrees Celcius\n\n\n\n# get the 10th percentile for the city you chose\npercentile_10 = TA_temp['AvgTemp_C'].quantile(.1) \nprint(f\"The tenth percentile in {city} is: {percentile_10:.2f} degrees Celcius\")\n\nThe tenth percentile in Tel Aviv is: 14.17 degrees Celcius\n\n\n\n# get the 90th percentile for the city you chose\npercentile_90 = TA_temp['AvgTemp_C'].quantile(.9) \nprint(f\"The ninetieth percentile in {city} is: {percentile_90:.2f} degrees Celcius\")\n\nThe ninetieth percentile in Tel Aviv is: 27.67 degrees Celcius\n\n\n\n# begin plotting\nsns.set_theme(style=\"whitegrid\")\n\n\n# make a box plot of temperature for the city you chose\nfig, ax = plt.subplots()\n# ax = sns.boxplot(x=TA_temp.AvgTemp_C)\nsns.boxplot(x=TA_temp['AvgTemp_C'], ax=ax)\nax.set(xlabel=f'Average Daily Temperature in {city}')\n\n[Text(0.5, 0, 'Average Daily Temperature in Tel Aviv')]\n\n\n\n\n\n\n# compare all the cities\n\n# clean data\ntemp_df = temp_df.loc[temp_df['AvgTemperature']&gt;(-50)]\ntemp_df['AvgTemp_C'] = (temp_df['AvgTemperature'] - 32)*(5/9)\n\n# plot\nfig, ax = plt.subplots()\nsns.boxplot(x=temp_df['AvgTemp_C'], y=temp_df['City'], ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_82655/3601528637.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  temp_df['AvgTemp_C'] = (temp_df['AvgTemperature'] - 32)*(5/9)\n\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# make a violin plot of the temperature\nfig, ax = plt.subplots()\nsns.violinplot(x=temp_df['AvgTemp_C'], y=temp_df['City'], ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]"
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#try-out",
    "href": "tirgul/tirgul_2/tirgul_2.html#try-out",
    "title": "2  Tutorial 2",
    "section": "2.4 Try out",
    "text": "2.4 Try out\n\nplot the Average Daily Temperature of the year 2000 for all cities\nplot the Average Daily Temperature of January 1st for all cities in all years\n\n\n\nAnswer for 1\nmilenium = temp_df.loc[temp_df['Year'] == 2000]\n# plot\nfig, ax = plt.subplots()\nsns.boxplot(data=milenium, x='AvgTemp_C', y='City', ax=ax)\nax.set(xlabel='Average Daily Temperature in 2000')\n\n\n[Text(0.5, 0, 'Average Daily Temperature in 2000')]\n\n\n\n\n\n\n\nAnswer for 2\njan_df = temp_df.loc[(temp_df['Day'] == 1)&((temp_df['Month'] == 1))]\n# plot\nfig, ax = plt.subplots()\nsns.boxplot(data=jan_df, x='AvgTemp_C', y='City', ax=ax)\nax.set(xlabel='Average Daily Temperature on Jan 1st')\n\n\n[Text(0.5, 0, 'Average Daily Temperature on Jan 1st')]\n\n\n\n\n\n\n# make a histogram of the data for the city you chose\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp['AvgTemp_C'], ax=ax)\nax.set(xlabel=f'Average Daily Temperature in {city}')\n\n[Text(0.5, 0, 'Average Daily Temperature in Tel Aviv')]\n\n\n\n\n\n\n# play around with the bin size for the histogram -- try more bins\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp['AvgTemp_C'], bins=100, ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# now fewer bins\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp.AvgTemp_C, bins=10, ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# add kernel density estimator\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp.AvgTemp_C, bins=20, kde = True, ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# how can we normalize the histogram data?\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp['AvgTemp_C'], bins=20, kde=True, stat=\"density\", ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\nwhat’s the difference between the “density” stat and “probablity” stat? read the documentation.\n\n# plotting 2 side by side\nfig, ax = plt.subplots(1,2, sharey=True)\n\nsns.histplot(x=TA_temp.AvgTemp_C, bins=20, kde=True, stat=\"probability\", ax=ax[0])\nax[0].set(title='propability', xlabel='Average Daily Temperature')\n\nsns.histplot(x=TA_temp.AvgTemp_C, bins=20, kde=True, stat=\"density\", ax=ax[1])\nax[1].set(title='density', xlabel='Average Daily Temperature')\n\n[Text(0.5, 1.0, 'density'), Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# add the mean to the plot\nmean_temp = TA_temp['AvgTemp_C'].mean() \n\nfig, ax = plt.subplots()\nsns.histplot(x=TA_temp['AvgTemp_C'], \n             bins=20,\n             kde=True,\n             stat=\"probability\",\n             ax=ax,\n             )\nax.set(xlabel='Average Daily Temperature')\nax.axvline(mean_temp, label='mean', color='r')\nax.text(mean_temp,0.14, 'mean', va='bottom',\n        ha='center', fontsize=14, weight='bold', color='r') \n\nText(21.249324753733717, 0.14, 'mean')"
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#try-out-1",
    "href": "tirgul/tirgul_2/tirgul_2.html#try-out-1",
    "title": "2  Tutorial 2",
    "section": "2.5 Try out",
    "text": "2.5 Try out\n\nadd the mode\nadd the median\n\n\n# make a histogram of the data\nfig, ax = plt.subplots()\nsns.histplot(x=temp_df['AvgTemp_C'], hue=temp_df['City'], ax=ax)\nax.set(xlabel='Average Daily Temperature')\n\n[Text(0.5, 0, 'Average Daily Temperature')]\n\n\n\n\n\n\n# another type of histogram\nfig, ax = plt.subplots()\nsns.histplot(x=temp_df['AvgTemp_C'], y=temp_df['City'], \n             hue=temp_df['City'], legend=False, ax=ax)\n\n&lt;Axes: xlabel='AvgTemp_C', ylabel='City'&gt;"
  },
  {
    "objectID": "tirgul/tirgul_2/tirgul_2.html#encoding-categorical-variables",
    "href": "tirgul/tirgul_2/tirgul_2.html#encoding-categorical-variables",
    "title": "2  Tutorial 2",
    "section": "2.6 Encoding categorical variables",
    "text": "2.6 Encoding categorical variables\nSometimes, for reasons that will be clear on the HW, we’ll want to encode our categorical variables so that they are numbers instead.\nThere are many ways that we can achive this.\nHere will learn one, for more examples see: https://pbpython.com/categorical-encoding.html\n\n# what are our cities again?\ntemp_df['City'].unique()\n\narray(['Lilongwe', 'Capetown', 'Tel Aviv', 'Amman', 'Beirut', 'Rochester'],\n      dtype=object)\n\n\n\n# dictionary for encoding cities (note: we can encode more than one variable at a time)\ncleanup_cities = {\"City\": {\"Lilongwe\": 1, \n                           \"Capetown\": 2,\n                           \"Tel Aviv\": 3,\n                           \"Amman\": 4,\n                           \"Beirut\": 5,\n                           \"Rochester\": 6}}\n\n\n# new dataframe with encoded values\ntemp_df_encoded = temp_df.replace(cleanup_cities)\ntemp_df_encoded.dtypes\n\nCountry            object\nCity                int64\nMonth               int64\nDay                 int64\nYear                int64\nAvgTemperature    float64\nAvgTemp_C         float64\ndtype: object\n\n\n\n# option 2 -- use Pandas\n\n# what are our data types?\ntemp_df.dtypes\n\nCountry            object\nCity               object\nMonth               int64\nDay                 int64\nYear                int64\nAvgTemperature    float64\nAvgTemp_C         float64\ndtype: object\n\n\n\n# assign city to be a categorical variable\ntemp_df[\"City\"] = temp_df[\"City\"].astype('category')\ntemp_df.dtypes\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_82655/1724604918.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  temp_df[\"City\"] = temp_df[\"City\"].astype('category')\n\n\nCountry             object\nCity              category\nMonth                int64\nDay                  int64\nYear                 int64\nAvgTemperature     float64\nAvgTemp_C          float64\ndtype: object\n\n\n\n# use codes to encode variable\ntemp_df[\"City_encoded\"] = temp_df[\"City\"].cat.codes\ntemp_df.dtypes\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_82655/3917241889.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  temp_df[\"City_encoded\"] = temp_df[\"City\"].cat.codes\n\n\nCountry             object\nCity              category\nMonth                int64\nDay                  int64\nYear                 int64\nAvgTemperature     float64\nAvgTemp_C          float64\nCity_encoded          int8\ndtype: object"
  },
  {
    "objectID": "tirgul/tirgul_3/tirgul_3.html#principal-component-analysis",
    "href": "tirgul/tirgul_3/tirgul_3.html#principal-component-analysis",
    "title": "3  Tutorial 3",
    "section": "3.1 Principal Component Analysis",
    "text": "3.1 Principal Component Analysis\nPrincipal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.\nIt can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.\nThe PCA method can be described and implemented using the tools of linear algebra.\nPCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B.\nBy default, PCA creates as many axes as there are dimensions to a given dataset but it ranks these directions, called principal components (PCs), in the order of importance: the first PC always captures the most amount of data variance possible, the second one shows the second-largest amount of variance, and so forth.\nAfter the data has been projected into this new subspace, we might drop out some of the axes and thus reduce dimensionality without losing much important information.\nFor more details on the steps:\nhttps://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n\n3.1.1 How does PCA work?\n\nImagine this is our dataset that we’re trying to cluster; we only have two dimensions.\n\n\n%matplotlib inline\nfrom IPython.display import Image\nImage('image_1.png')\n\n\n\n\n\nIf we project the data onto the horizontal axis (our attribute 1) we won’t see much spread; it will show a nearly equal distribution of the observations.\n\n\nImage('image_2.png')\n\n\n\n\n\nAttribute 2, obviously, isn’t hugely helpful either.\n\n\nImage('image_3.png')\n\n\n\n\n\nEvidently, the data points in our case are spreading diagonally so we need a new line that would better capture this.\n\n\nImage('image_4.png')\n\n\n\n\n\nThe second PC must represent the second maximum amount of variance; it’s going to be a line that’s orthogonal to our first axis. *Due to PCA’s math being based on eigenvectors and eigenvalues, new principal components will always come out orthogonal to the ones before them.\n\n\nImage('image_5.png')\n\n\n\n\n\n3.1.1.1 Important!\nBefore applying PCA, we must ensure that all our attributes (dimensions) are centered around zero and have a standard deviation of 1. The method won’t work if we have entirely different scales for our data."
  },
  {
    "objectID": "tirgul/tirgul_3/tirgul_3.html#example",
    "href": "tirgul/tirgul_3/tirgul_3.html#example",
    "title": "3  Tutorial 3",
    "section": "3.2 Example",
    "text": "3.2 Example\nhttps://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n\n# import packages \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\n\n# create and plot some random data \nrng = np.random.RandomState(1)\nX = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\nplt.scatter(X[:, 0], X[:, 1])\nplt.axis('equal');\n\n\n\n\n\n# use sklearn to do PCA\npca = PCA(n_components=2)\npca.fit(X);\n\npca.components_ returns the direction of each principal component, sorted by decreasing explained variance. These are the eigenvectors of the decomposition.\n\n# get components\nprint(pca.components_)\n\n[[-0.94446029 -0.32862557]\n [-0.32862557  0.94446029]]\n\n\npca.explained_variance_ returns the variance explained by each component, sorted by decreasing magnitude. These are the eigenvalues of the decomposition. They do not sum up to 1.\n\n# get explained variance\nprint(pca.explained_variance_)\n\n[0.7625315 0.0184779]\n\n\npca.explained_variance_ratio_ returns the proportion of total variance explained by each component. They sum up to 1. These are computed as:\nexplained_variance_ratio_[i] = explained_variance_[i] / sum(explained_variance_)  \n\n# get explained variance ratio\nprint(pca.explained_variance_ratio_)\n\nTo see what these numbers mean, let’s visualize them as vectors over the input data, using the “components” to define the direction of the vector, and the “explained variance” to define the squared-length of the vector.\n\ndef draw_vector(v0, v1, ax=None):\n    ax = ax or plt.gca()\n    arrowprops=dict(arrowstyle='-&gt;',\n                    linewidth=2,\n                    shrinkA=0, shrinkB=0)\n    ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n# plot data\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2)\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    draw_vector(pca.mean_, pca.mean_ + v)\nplt.axis('equal');\n\n\n\n\nThese vectors represent the principal axes of the data, and the length of the vector is an indication of how “important” that axis is in describing the distribution of the data—more precisely, it is a measure of the variance of the data when projected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data.\n\n3.2.1 Dimensional reduction\nUsing PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n\npca = PCA(n_components=1)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)\n\noriginal shape:    (200, 2)\ntransformed shape: (200, 1)\n\n\nThe transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data.\n\nX_new = pca.inverse_transform(X_pca)\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2)\nplt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\nplt.axis('equal');\n\n\n\n\nThe light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much “information” is discarded in this reduction of dimensionality.\nThis reduced-dimension dataset is in some senses “good enough” to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.\n\n\n3.2.2 Example 2: PCA to Speed-up Machine Learning Algorithms\nThe MNIST database of handwritten digits has 784 feature columns (784 dimensions).\nWe can break it into a training set of 60,000 examples, and a test set of 10,000 examples.\nhttps://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n\n# get MNIST dataset\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784')\n\n/opt/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\n\n\n# the data consists of images -- there are 70,000 images with 784 dimensions (784 features)\nmnist.data.shape\n\n(70000, 784)\n\n\n\n# each image is a number\nmnist.target[0:10]\n\n0    5\n1    0\n2    4\n3    1\n4    9\n5    2\n6    1\n7    3\n8    1\n9    4\nName: class, dtype: category\nCategories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n\n\n\n# split into training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# test_size: what proportion of original data is used for test set\ntest_size = 1/7.0\n(train_img, \n test_img, \n train_lbl, \n test_lbl) = train_test_split(mnist.data, \n                              mnist.target, \n                              test_size=test_size, \n                              random_state=0)\n\n\n# standardize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# fit on training set only.\nscaler.fit(train_img)\n\n# apply transform to both the training set and the test set.\ntrain_img = scaler.transform(train_img)\ntest_img = scaler.transform(test_img)\n\n\n\n3.2.3 Apply PCA\nNotice the code below uses PCA(0.95) for the number of components parameter. This means that scikit-learn will automatically choose the minimum number of principal components such that the cumulative sum of the explained_variance_ratio_ is at least 95%. In other words, the selected components together retain 95% of the total variance present in the original data.\n\n# make an instance of the model\npca = PCA(.95)\n\n\n# fit on training data set\npca.fit(train_img)\n\nPCA(n_components=0.95)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=0.95)\n\n\n\n# how many components are there?\npca.n_components_\n\n327"
  },
  {
    "objectID": "tirgul/tirgul_3/tirgul_3.html#what-do-we-gain-from-this",
    "href": "tirgul/tirgul_3/tirgul_3.html#what-do-we-gain-from-this",
    "title": "3  Tutorial 3",
    "section": "3.3 What do we gain from this?",
    "text": "3.3 What do we gain from this?\n\n# apply the transform to both the training set and the test set.\ntrain_img = pca.transform(train_img)\ntest_img = pca.transform(test_img)\n\n\n# let's make a logistic model\nfrom sklearn.linear_model import LogisticRegression\n\n# default solver is incredibly slow so changed to 'lbfgs'\nlogisticRegr = LogisticRegression(solver = 'lbfgs', max_iter=1000)\n\n# train model\nlogisticRegr.fit(train_img, train_lbl)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n\n# predict for multiple observations (images) at once\nprint(f'Predicted\\t{logisticRegr.predict(test_img[0:10])}')\nprint(f'Observed\\t{np.array(test_lbl[0:10])}')\n\nPredicted   ['0' '4' '1' '2' '4' '7' '7' '1' '1' '7']\nObserved    ['0' '4' '1' '2' '7' '9' '7' '1' '1' '7']\n\n\n\n# how does the model do?\nscore = logisticRegr.score(test_img, test_lbl)\nscore\n\n0.9184\n\n\nhow does changing the amount of variance affect the model?\n\npd.DataFrame(data = [[1.00, 784, 48.94, .9158],\n                     [.99, 541, 34.69, .9169],\n                     [.95, 330, 13.89, .92],\n                     [.90, 236, 10.56, .9168],\n                     [.85, 184, 8.85, .9156]], \n             columns = ['Variance Retained',\n                      'Number of Components', \n                      'Time (seconds)',\n                      'Accuracy'])\n\n\n\n\n\n\n\n\nVariance Retained\nNumber of Components\nTime (seconds)\nAccuracy\n\n\n\n\n0\n1.00\n784\n48.94\n0.9158\n\n\n1\n0.99\n541\n34.69\n0.9169\n\n\n2\n0.95\n330\n13.89\n0.9200\n\n\n3\n0.90\n236\n10.56\n0.9168\n\n\n4\n0.85\n184\n8.85\n0.9156"
  },
  {
    "objectID": "tirgul/tirgul_3/tirgul_3.html#looking-at-the-data-a-more-broadly---pair-plot-clustering-ploting-pca",
    "href": "tirgul/tirgul_3/tirgul_3.html#looking-at-the-data-a-more-broadly---pair-plot-clustering-ploting-pca",
    "title": "3  Tutorial 3",
    "section": "3.4 Looking at the data a more broadly - pair plot, clustering, ploting PCA",
    "text": "3.4 Looking at the data a more broadly - pair plot, clustering, ploting PCA\nseaborn has some powerful functions for visualisation of the whole dataset\n\n# import packages \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport pandas as pd\n\nLoad the breast cancer dataset. In this dataset features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\n\ncancer_data = load_breast_cancer()\ndata = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\nall_data = data.copy()\nall_data['target'] = cancer_data.target\n\n\ndata.columns\n\nIndex(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error', 'fractal dimension error',\n       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n       'worst smoothness', 'worst compactness', 'worst concavity',\n       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n      dtype='object')\n\n\n\n3.4.1 Pair plot\nA pair plot in Seaborn is a grid of scatterplots and histograms showing the pairwise relationships and distributions of multiple variables in a dataset. It can be used to visualize patterns and correlations between variables, identify potential outliers or clusters, and guide further analysis.\n\nsns.pairplot(data=all_data[['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness','target']], hue='target',corner=True, markers='+')\n\n\n\n\n\n\n3.4.2 Clustermap\nA clustermap in Seaborn is a heatmap-like plot that arranges rows and columns of a dataset based on their similarity and creates dendrograms to show hierarchical clustering. It can be used to explore patterns and relationships in high-dimensional data, identify groups or clusters of similar observations, and guide feature selection or dimensionality reduction.\n\n# first we standardize the data by computing the z score \nz = data.apply(lambda x:(x-x.mean())/x.std(), axis=0)\n\n\n# Ploting the clustermap\nlut = dict(zip(all_data['target'].unique(), \"rbg\"))\nrow_colors = all_data['target'].map(lut)\n# sns.clustermap(z, row_colors=row_colors)\n\nsns.clustermap(z, row_colors=row_colors, vmax=10, vmin=-10, cmap='bwr')\n\n# another way - instead of manualy calculating the z score you can set the z_score arg to 1\n# sns.clustermap(data, row_colors=row_colors, z_score=1)\n\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/matrix.py:560: UserWarning: Clustering large matrix with scipy. Installing `fastcluster` may give better performance.\n  warnings.warn(msg)\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/matrix.py:560: UserWarning: Clustering large matrix with scipy. Installing `fastcluster` may give better performance.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n3.4.3 Ploting PCA\n\npca = PCA().fit(z)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n\n\n\n\n\nX = z \npca = PCA().fit(X)\nX_trans = pca.transform(X)\ncolors = all_data['target']\n\nsns.scatterplot(x=X_trans[:,0], y=X_trans[:,1], hue=colors, palette='deep')\n\nplt.xlabel ('PC1 ({:.2f}%)'.format(100*pca.explained_variance_ratio_[0]))\nplt.ylabel ('PC2 ({:.2f}%)'.format(100*pca.explained_variance_ratio_[1]))\n\nText(0, 0.5, 'PC2 (18.97%)')"
  },
  {
    "objectID": "tirgul/tirgul_4/tirgul_4.html",
    "href": "tirgul/tirgul_4/tirgul_4.html",
    "title": "4  Tutorial 4",
    "section": "",
    "text": "5 Hierarchical Clustering\nHierarchical clustering is a type of unsupervised machine learning algorithm used to cluster unlabeled data points. Hierarchical clustering groups together data points with similar characteristics.\nThere are two types of hierarchical clustering: agglomerative and divisive. * Agglomerative: data points are clustered using a bottom-up approach starting with individual data points * Divisive: all the data points are treated as one big cluster and the clustering process involves dividing the one big cluster into several small clusters.\nToday, we’ll focus on agglomerative clustering.\nhttps://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\nhttps://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn\nhttps://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\nUse the dataset cluster_play_data.csv to play with the clustering method we learned. download .csv here\nFirst step hint - try to find out on what columns to preform the clustering.\nAnswer part 1\n# import data\n\n# # uncomment below:\n# play = pd.read_csv('cluster_play_data.csv')\n# play.head()\nAnswer part 2\n# # uncomment below:\n# sns.pairplot(play)\nAnswer part 3\n# # uncomment below:\n# # examine dendogram\n# # isolate data from annual income and spending score columns\n# # data = customer_data.iloc[:, 3:5].values\n# data = play[['A','C']].values\n# plt.figure()\n# plt.title(\"Customer Dendograms\")\n# plt.xlabel('Points')\n# plt.ylabel('Euclidean Distance')\n# dend = shc.dendrogram(shc.linkage(data, method='ward'))\nAnswer part 4\n# # uncomment below:\n# # create clusters\n# cluster = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')\n# cluster.fit_predict(data)\n\n# # look at the clusters\n# plt.figure(figsize=(10, 7))\n# plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')\n# plt.xlabel('Annual Income (k$)')\n# plt.ylabel('Spending Score (1-100)')"
  },
  {
    "objectID": "tirgul/tirgul_4/tirgul_4.html#topics",
    "href": "tirgul/tirgul_4/tirgul_4.html#topics",
    "title": "4  Tutorial 4",
    "section": "4.1 Topics",
    "text": "4.1 Topics\n\nK-means clustering\nHierarchical clustering"
  },
  {
    "objectID": "tirgul/tirgul_4/tirgul_4.html#k-means-clustering",
    "href": "tirgul/tirgul_4/tirgul_4.html#k-means-clustering",
    "title": "4  Tutorial 4",
    "section": "4.2 K-Means Clustering",
    "text": "4.2 K-Means Clustering\nThe k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. There are many different types of clustering methods, but k-means is one of the oldest and most approachable. These traits make implementing k-means clustering in Python reasonably straightforward, even for novice programmers and data scientists.\nThe k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\nThe “cluster center” is the arithmetic mean of all the points belonging to the cluster. Each point is closer to its own cluster center than to other cluster centers.\nhttps://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\nhttps://realpython.com/k-means-clustering-python/#:~:text=The%20k%2Dmeans%20clustering%20method,data%20objects%20in%20a%20dataset.&text=You’ll%20walk%20through%20an,the%20data%20to%20evaluating%20results."
  },
  {
    "objectID": "tirgul/tirgul_4/tirgul_4.html#example",
    "href": "tirgul/tirgul_4/tirgul_4.html#example",
    "title": "4  Tutorial 4",
    "section": "4.3 Example",
    "text": "4.3 Example\n\n# import packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import fcluster\n\n\n4.3.1 Starting from the end\nBefore getting to the algorithm itself, let’s see what K-Means actually does\n\n\n4.3.2 Create random data grouped in four “blobs”\n\nX, y_true = make_blobs(n_samples=300, \n                       centers=4,\n                       cluster_std=0.60, \n                       random_state=0)\nplt.scatter(X[:, 0],\n            X[:, 1],\n            s=50);\n\n\n\n\n\n\n4.3.3 Calculate K-Means\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\nclusters_kmeans = kmeans.predict(X)\nclusters_kmeans\n\n/opt/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\narray([2, 1, 0, 1, 2, 2, 3, 0, 1, 1, 3, 1, 0, 1, 2, 0, 0, 2, 3, 3, 2, 2,\n       0, 3, 3, 0, 2, 0, 3, 0, 1, 1, 0, 1, 1, 1, 1, 1, 3, 2, 0, 3, 0, 0,\n       3, 3, 1, 3, 1, 2, 3, 2, 1, 2, 2, 3, 1, 3, 1, 2, 1, 0, 1, 3, 3, 3,\n       1, 2, 1, 3, 0, 3, 1, 3, 3, 1, 3, 0, 2, 1, 2, 0, 2, 2, 1, 0, 2, 0,\n       1, 1, 0, 2, 1, 3, 3, 0, 2, 2, 0, 3, 1, 2, 1, 2, 0, 2, 2, 0, 1, 0,\n       3, 3, 2, 1, 2, 0, 1, 2, 2, 0, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 1, 3,\n       3, 2, 1, 3, 3, 1, 0, 1, 1, 3, 0, 3, 0, 3, 1, 0, 1, 1, 1, 0, 1, 0,\n       2, 3, 1, 3, 2, 0, 1, 0, 0, 2, 0, 3, 3, 0, 2, 0, 0, 1, 2, 0, 3, 1,\n       2, 2, 0, 3, 2, 0, 3, 3, 0, 0, 0, 0, 2, 1, 0, 3, 0, 0, 3, 3, 3, 0,\n       3, 1, 0, 3, 2, 3, 0, 1, 3, 1, 0, 1, 0, 3, 0, 0, 1, 3, 3, 2, 2, 0,\n       1, 2, 2, 3, 2, 3, 0, 1, 1, 0, 0, 1, 0, 2, 3, 0, 2, 3, 1, 3, 2, 0,\n       2, 1, 1, 1, 1, 3, 3, 1, 0, 3, 2, 0, 3, 3, 3, 2, 2, 1, 0, 0, 3, 2,\n       1, 3, 0, 1, 0, 2, 2, 3, 3, 0, 2, 2, 2, 0, 1, 1, 2, 2, 0, 2, 2, 2,\n       1, 3, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 1, 3], dtype=int32)\n\n\n\nlen(clusters_kmeans)\n\n300\n\n\n\n\n4.3.4 Plot results\n\n# blobs\nplt.scatter(X[:, 0], \n            X[:, 1], \n            c=clusters_kmeans, \n            s=50, \n            cmap='viridis')\n\n# center of blob\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], \n            centers[:, 1], \n            c='red', \n            s=200, \n            alpha=0.9)\n\n&lt;matplotlib.collections.PathCollection at 0x125c68b90&gt;\n\n\n\n\n\n\n\n4.3.5 How does the algorithm work?\nK-Means uses the Expectation-Maximization Algorithm\nHow does this work?\n\nGuess some cluster centers\nRepeat until converged\n\nE-Step: assign points to the nearest cluster center\nM-Step: set the cluster centers to the mean\n\n\nThe “E-step” or “Expectation step” involves updating our expectation of which cluster each point belongs to.\nThe “M-step” or “Maximization step” involves maximizing some fitness function that defines the location of the cluster centers. In this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n\n%matplotlib inline\nfrom IPython.display import Image\nImage('image_1.png')\n\n\n\n\n\n\n4.3.6 Visualisation\nHere is a cool video for visualising the prosses of K-means clustering.\n\n\n4.3.7 Choosing the right number of clusters\nWe’ll look at two methods for choosing the correct number of clusters.\nThe elbow method and the silhouette method. We can use these methods together. No need to pick just one.\nThe quality of the cluster assignments is determined by computing the sum of the squared error (SSE) after the centroids converge.\nSSE is defined as the sum of the squared distance between centroid and each member of the cluster.\n\n\n4.3.8 The elbow method\nTo perform the elbow method, run several k-means, increment k with each iteration, and record the SSE.\n\n# loop over different numbers of clusters\nkmeans_kwargs = {\"init\": \"random\",\n                 \"n_init\": 10,\n                 \"max_iter\": 300,\n                 \"random_state\": 42,}\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X)\n    error = kmeans.inertia_\n    sse.append(kmeans.inertia_)\n    print(f\"With {k} clusters the SSE was {error}\")\n\nWith 1 clusters the SSE was 2812.137595303235\nWith 2 clusters the SSE was 1190.7823593643443\nWith 3 clusters the SSE was 546.8911504626299\nWith 4 clusters the SSE was 212.00599621083478\nWith 5 clusters the SSE was 188.77323556773723\nWith 6 clusters the SSE was 170.94840955438684\nWith 7 clusters the SSE was 154.8848618157605\nWith 8 clusters the SSE was 139.20927769246356\nWith 9 clusters the SSE was 126.56204002887225\nWith 10 clusters the SSE was 111.8591054874254\n\n\n\n# plot the data\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\n\n\n\n\nDetermining the elbow point in the SSE curve isn’t always straightforward. If you’re having trouble choosing the elbow point of the curve, then you could use a Python package, kneed, to identify the elbow point programmatically.\n\n\n4.3.9 The silhouette coefficient\nThe silhouette coefficient is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors: 1. How close the data point is to other points in the cluster 2. How far away the data point is from points in other clusters\nSilhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters.\n\n# A list holds the silhouette coefficients for each k\nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X)\n    score = silhouette_score(X, kmeans.labels_)\n    silhouette_coefficients.append(score)\n    print(f\"With {k} clusters the score was {score}\")\n\nWith 2 clusters the score was 0.5426422297358303\nWith 3 clusters the score was 0.5890390393551768\nWith 4 clusters the score was 0.6819938690643478\nWith 5 clusters the score was 0.5923875148758644\nWith 6 clusters the score was 0.49563409602576686\nWith 7 clusters the score was 0.4277257665723784\nWith 8 clusters the score was 0.32252064195197455\nWith 9 clusters the score was 0.3335178002757919\nWith 10 clusters the score was 0.34598612018426733\n\n\n\nplt.plot(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()"
  },
  {
    "objectID": "tirgul/tirgul_5/tirgul_5.html",
    "href": "tirgul/tirgul_5/tirgul_5.html",
    "title": "5  Tutorial 5",
    "section": "",
    "text": "6 Bootstrapping\nBootstrapping is a statistical method that involves repeatedly resampling a dataset with replacement to estimate the distribution of a statistic. This technique allows for the assessment of the accuracy and variability of sample estimates, such as the mean or standard deviation, without making strict assumptions about the underlying population distribution. By generating many resampled datasets (called bootstrap samples) and calculating the statistic of interest for each sample, bootstrapping provides an empirical approximation of the sampling distribution. This method is particularly useful when dealing with small samples or when traditional parametric assumptions cannot be applied.\nLet’s create a numpy array to represent heights of females at the Faculta\nnp.random.seed(42)\nmean = 162.2\nstd = 5.5\nsample_size = 1000\nheights = np.random.normal(loc=mean, \n                           scale=std, \n                           size=sample_size)\n\n# sample mean\nmean_value = heights.mean()\n\n# population standard deviation\nstd = heights.std()\n\nprint(f\"The population's mean value is {mean_value}\\nThe standard deviation is {std}\")\n\nThe population's mean value is 162.30632630702277\nThe standard deviation is 5.382994142610448\nNow we can construct a simulated sampling distribution\nboot_straps = 500\nsample_size\nsample_means = np.zeros(boot_straps)\nfor ii in range(boot_straps):\n    sample = np.random.choice(heights, \n                              size=sample_size,\n                              replace=True)\n    sample_means[ii]= sample.mean()\n# now we can find the standard deviation of the means\nse_mean_height_bootstrap = sample_means.std()\nprint(f\"The standard error of the mean calculated using bootstrapping is: {se_mean_height_bootstrap:.3f}\")\n\nThe standard error of the mean calculated using bootstrapping is: 0.175\nReminder, for the mean, it is also possible to estimate the Standard Error (SE) of the mean by dividing the standard deviation (STD) of the values by the square root of the sample size. The formula for the Standard Error of the Mean (SEM) is:\n\\text{SEM} = \\frac{\\sigma}{\\sqrt{n}}\nwhere \\sigma is the standard deviation of the sample, and n is the sample size.\nFor other statistics, such as the median, there is no such analytical formula. The utility of the bootstrap method lies in its ability to estimate the SE of these other statistics. By repeatedly resampling the data with replacement and calculating the statistic of interest for each resample, bootstrapping provides an empirical distribution of the statistic. From this distribution, we can estimate the SE and other measures of variability.\nse_mean_height_analytical = heights.std()/np.sqrt(sample_size)\nprint(f\"The standard error of the mean calculated using the analytical formula is: {se_mean_height_analytical:.3f}\")\n\nThe standard error of the mean calculated using the analytical formula is: 0.170"
  },
  {
    "objectID": "tirgul/tirgul_5/tirgul_5.html#topics",
    "href": "tirgul/tirgul_5/tirgul_5.html#topics",
    "title": "5  Tutorial 5",
    "section": "5.1 Topics",
    "text": "5.1 Topics\n\nStandard error vs. Standard deviation\nBoostrapping\nConfidence Intervals"
  },
  {
    "objectID": "tirgul/tirgul_5/tirgul_5.html#standard-error-vs.-standard-deviation",
    "href": "tirgul/tirgul_5/tirgul_5.html#standard-error-vs.-standard-deviation",
    "title": "5  Tutorial 5",
    "section": "5.2 Standard error vs. standard deviation",
    "text": "5.2 Standard error vs. standard deviation\n\nimport numpy as np\nimport seaborn as sns\n\nnp.random.seed(42)\nsns.set_theme(style=\"whitegrid\")\n\nLet’s define a “population” of size 1,000,000. Each member of the population will be a number from the standard normal distribution. We will be using the numpy function randn\n\npopulation = np.random.randn(1000000)\n\n\npopulation\n\narray([ 0.49671415, -0.1382643 ,  0.64768854, ..., -0.11297975,\n        1.46914237,  0.47643025])\n\n\nWhat’s the population’s mean and standard deviation?\n\nmean_value = population.mean()\nstd = population.std()\nprint(f\"The mean value is {mean_value} and the standard deviation is {std}\")\n\nThe mean value is -0.0015997564542563718 and the standard deviation is 1.000188036804731\n\n\nWhat does the distribution of the population look like?\n\nsns.displot(population)\n\n\n\n\nIf this were the real world, sampling 1,000,000 people would be difficult. Let’s say that our budget allowed us to sample only 100 people at a time. How can we simulate that?\n\nsample_size = 100\nsample_100 = np.random.choice(population, sample_size)\n\nWhat’s the sample’s mean and standard deviation?\n\nmean_100 = sample_100.mean()\nstd_100 = sample_100.std()\nprint(f\"The mean value is {mean_100}\\nThe standard deviation is {std_100}\")\n\nThe mean value is -0.1753180501180135\nThe standard deviation is 1.1706030198688049\n\n\nNotice that our sample’s mean value is pretty different from that of the population. What if we take a bigger sample size?\n\nsample_size = 1000\nsample_1000 = np.random.choice(population, sample_size)\nmean_1000 = sample_1000.mean()\nstd_1000 = sample_1000.std()\nprint(f\"The mean value is {mean_1000}\\nThe standard deviation is {std_1000}\")\n\nThe mean value is -0.008352574043024205\nThe standard deviation is 1.003117759041072\n\n\nOr smaller…\n\nsample_size = 5\nsample_5 = np.random.choice(population, sample_size)\nmean_5 = sample_5.mean()\nstd_5 = sample_5.std()\nprint(f\"The mean value is {mean_5}\\nThe standard deviation is {std_5}\")\n\nThe mean value is 0.03172109602313291\nThe standard deviation is 0.758566765992809\n\n\nThe standard error of the mean (SEM) measures the precision of the estimate of the sample mean. We can use the formula:\n\\text{SEM} = \\frac{\\sigma}{\\sqrt{n}}\n\n\\sigma = the standard deviation of the sample\nn = the sample size\n\nNow we can compare our standard errors using the different sample sizes.\n\nse_5 = std_5/np.sqrt(5)\nse_100 = std_100/np.sqrt(100)\nse_1000 = std_1000/np.sqrt(1000)\n\nprint(f\"The SEM when the sample size was 5 was {se_5}\")\nprint(f\"The SEM when the sample size was 100 was {se_100}\")\nprint(f\"The SEM when the sample size was 1000 was {se_1000}\")\n\nThe SEM when the sample size was 5 was 0.3392413708464193\nThe SEM when the sample size was 100 was 0.11706030198688049\nThe SEM when the sample size was 1000 was 0.03172136879933749"
  },
  {
    "objectID": "tirgul/tirgul_5/tirgul_5.html#confidence-intervals",
    "href": "tirgul/tirgul_5/tirgul_5.html#confidence-intervals",
    "title": "5  Tutorial 5",
    "section": "6.1 Confidence Intervals",
    "text": "6.1 Confidence Intervals\nWe can also compute the confidence interval for our estimate of the mean. Let’s say we want the 90 percent confidence interval.\n\nconf_level = 0.9\nlow_bound = (1-conf_level)/2\nup_bound = (1+conf_level)/2\nCI = (np.quantile(sample_means, low_bound),\n      np.quantile(sample_means, up_bound)) \n\n\nprint(f\"The lower bound of the confidence interval is: {CI[0]}\")\nprint(f\"The upper bound of the confidence interval is: {CI[1]}\")\n\nThe lower bound of the confidence interval is: 162.0432374294097\nThe upper bound of the confidence interval is: 162.6211799101076"
  },
  {
    "objectID": "tirgul/tirgul_5/tirgul_5.html#can-you-bootsrap-confidence-interval-for-the-median",
    "href": "tirgul/tirgul_5/tirgul_5.html#can-you-bootsrap-confidence-interval-for-the-median",
    "title": "5  Tutorial 5",
    "section": "6.2 Can you bootsrap confidence interval for the median?",
    "text": "6.2 Can you bootsrap confidence interval for the median?\n\n\nAnswer\n# make a bootstrapped population of the median\nboot_straps = 500\nsample_size\nsample_medians = np.zeros(boot_straps)\nfor ii in range(boot_straps):\n    sample = np.random.choice(heights, \n                              size=sample_size,\n                              replace=True)\n    sample_medians[ii]= np.median(sample)\n\n# compute CI for the bootstrapped population of the median\nconf_level = 0.9\nlow_bound = (1-conf_level)/2\nup_bound = (1+conf_level)/2\nCI = (np.quantile(sample_medians, low_bound),\n      np.quantile(sample_medians, up_bound)) \n\n# # uncomment below:\n# print(f\"The lower bound of the confidence interval is: {CI[0]}\")\n# print(f\"The upper bound of the confidence interval is: {CI[1]}\")"
  },
  {
    "objectID": "tirgul/tirgul_6/tirgul_6.html",
    "href": "tirgul/tirgul_6/tirgul_6.html",
    "title": "6  Tutorial 6",
    "section": "",
    "text": "7 3. Using scipy.stats tests\nWe can compare the results above to a t-test using the scipy.stats library"
  },
  {
    "objectID": "tirgul/tirgul_6/tirgul_6.html#topics",
    "href": "tirgul/tirgul_6/tirgul_6.html#topics",
    "title": "6  Tutorial 6",
    "section": "6.1 Topics",
    "text": "6.1 Topics\n\nHypothesis testing using permutations\nt-test with the scipy.stats library"
  },
  {
    "objectID": "tirgul/tirgul_6/tirgul_6.html#hypothesis-testing",
    "href": "tirgul/tirgul_6/tirgul_6.html#hypothesis-testing",
    "title": "6  Tutorial 6",
    "section": "6.2 Hypothesis testing",
    "text": "6.2 Hypothesis testing\nA statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.\nhttps://en.wikipedia.org/wiki/Statistical_hypothesis_testing"
  },
  {
    "objectID": "tirgul/tirgul_6/tirgul_6.html#permutation-tests",
    "href": "tirgul/tirgul_6/tirgul_6.html#permutation-tests",
    "title": "6  Tutorial 6",
    "section": "6.3 Permutation tests",
    "text": "6.3 Permutation tests\nA permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of the observed data points\nhttps://en.wikipedia.org/wiki/Resampling_(statistics)#Permutation_tests\nTo illustrate the basic idea of a permutation test, suppose we collect random variables X_A and X_B for each individual from two groups A and B whose sample means are \\bar{x}_{A} and \\bar{x}_{B}, and that we want to know whether X_A and X_B come from the same distribution. Let n_{A} and n_{B} be the sample size collected from each group. The permutation test is designed to determine whether the observed difference between the sample means is large enough to reject, at some significance level, the null hypothesis H_{0} that the data drawn from A is from the same distribution as the data drawn from B.\nThe test proceeds as follows. First, the difference in means between the two samples is calculated: this is the observed value of the test statistic,T_\\text{obs}.\nNext, the observations of groups A and B are pooled, and the difference in sample means is calculated and recorded for every possible way of dividing the pooled values into two groups of size n_{A} and n_{B} (i.e., for every permutation of the group labels A and B). The set of these calculated differences is the exact distribution of possible differences (for this sample) under the null hypothesis that group labels are exchangeable (i.e., are randomly assigned).\nThe one-sided p-value of the test is calculated as the proportion of sampled permutations where the difference in means was greater than or equal to T_\\text{obs}. The two-sided p-value of the test is calculated as the proportion of sampled permutations where the [[absolute difference]] was greater than or equal to |T_\\text{obs}|.\nAlternatively, if the only purpose of the test is to reject or not reject the null hypothesis, one could sort the recorded differences, and then observe if T_\\text{obs} is contained within the middle (1 - \\alpha) \\times 100% of them, for some significance level \\alpha. If it is not, we reject the hypothesis of identical probability curves at the \\alpha\\times100\\% significance level.\n\n6.3.1 The Iris dataset\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters."
  },
  {
    "objectID": "tirgul/tirgul_6/tirgul_6.html#our-null-hypothesis",
    "href": "tirgul/tirgul_6/tirgul_6.html#our-null-hypothesis",
    "title": "6  Tutorial 6",
    "section": "6.4 Our Null Hypothesis",
    "text": "6.4 Our Null Hypothesis\n\nThere is no difference between mean sepal length for Iris virginca and Iris versicolor.\n\n\n# import packages\nimport numpy as np\nfrom sklearn import datasets\nimport pandas as pd\n\n\niris = datasets.load_iris()\ndf= pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                 columns= iris['feature_names'] + ['target'])\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\n0.0\nsetosa\n\n\n6\n4.6\n3.4\n1.4\n0.3\n0.0\nsetosa\n\n\n7\n5.0\n3.4\n1.5\n0.2\n0.0\nsetosa\n\n\n8\n4.4\n2.9\n1.4\n0.2\n0.0\nsetosa\n\n\n9\n4.9\n3.1\n1.5\n0.1\n0.0\nsetosa\n\n\n\n\n\n\n\nWe’ll use a permutation test to compare the mean sepal length for Iris virginca and Iris versicolor. We start by comparing the actual mean sepal length for the species we’re intrested in.\n\n# group by species\nby_species = df.groupby(\"species\")\nby_species.head(10)\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39706/2135608631.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  by_species = df.groupby(\"species\")\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\n0.0\nsetosa\n\n\n6\n4.6\n3.4\n1.4\n0.3\n0.0\nsetosa\n\n\n7\n5.0\n3.4\n1.5\n0.2\n0.0\nsetosa\n\n\n8\n4.4\n2.9\n1.4\n0.2\n0.0\nsetosa\n\n\n9\n4.9\n3.1\n1.5\n0.1\n0.0\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1.0\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1.0\nversicolor\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1.0\nversicolor\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1.0\nversicolor\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1.0\nversicolor\n\n\n55\n5.7\n2.8\n4.5\n1.3\n1.0\nversicolor\n\n\n56\n6.3\n3.3\n4.7\n1.6\n1.0\nversicolor\n\n\n57\n4.9\n2.4\n3.3\n1.0\n1.0\nversicolor\n\n\n58\n6.6\n2.9\n4.6\n1.3\n1.0\nversicolor\n\n\n59\n5.2\n2.7\n3.9\n1.4\n1.0\nversicolor\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2.0\nvirginica\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2.0\nvirginica\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2.0\nvirginica\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2.0\nvirginica\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2.0\nvirginica\n\n\n105\n7.6\n3.0\n6.6\n2.1\n2.0\nvirginica\n\n\n106\n4.9\n2.5\n4.5\n1.7\n2.0\nvirginica\n\n\n107\n7.3\n2.9\n6.3\n1.8\n2.0\nvirginica\n\n\n108\n6.7\n2.5\n5.8\n1.8\n2.0\nvirginica\n\n\n109\n7.2\n3.6\n6.1\n2.5\n2.0\nvirginica\n\n\n\n\n\n\n\nSeperate out the species we’re interested in\n\nvirginica = by_species.get_group(\"virginica\")\nversicolor = by_species.get_group(\"versicolor\")\n\nwork with only 10 rows of each species (this is done to make things more interesting because the more rows we look at the more significant result we will get in the end)\n\nn_rows = 10  # number of rows to keep should be less than 50\nvirginica = virginica[0:n_rows]\nversicolor = versicolor[0:n_rows]\n\nConcatenate the two species into one data frame\n\ndata = pd.concat([virginica, versicolor])\ndata\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nspecies\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2.0\nvirginica\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2.0\nvirginica\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2.0\nvirginica\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2.0\nvirginica\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2.0\nvirginica\n\n\n105\n7.6\n3.0\n6.6\n2.1\n2.0\nvirginica\n\n\n106\n4.9\n2.5\n4.5\n1.7\n2.0\nvirginica\n\n\n107\n7.3\n2.9\n6.3\n1.8\n2.0\nvirginica\n\n\n108\n6.7\n2.5\n5.8\n1.8\n2.0\nvirginica\n\n\n109\n7.2\n3.6\n6.1\n2.5\n2.0\nvirginica\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1.0\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1.0\nversicolor\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1.0\nversicolor\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1.0\nversicolor\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1.0\nversicolor\n\n\n55\n5.7\n2.8\n4.5\n1.3\n1.0\nversicolor\n\n\n56\n6.3\n3.3\n4.7\n1.6\n1.0\nversicolor\n\n\n57\n4.9\n2.4\n3.3\n1.0\n1.0\nversicolor\n\n\n58\n6.6\n2.9\n4.6\n1.3\n1.0\nversicolor\n\n\n59\n5.2\n2.7\n3.9\n1.4\n1.0\nversicolor\n\n\n\n\n\n\n\nGet the actual mean sepal length for each\n\ngrouped = data.groupby(\"species\", observed=True)['sepal length (cm)'].mean()\nprint(f'THe mean of sepal length for virginica is {grouped[\"virginica\"]}')\nprint(f'THe mean of sepal length for versicolor is {grouped[\"versicolor\"]}')\n\nTHe mean of sepal length for virginica is 6.57\nTHe mean of sepal length for versicolor is 6.1\n\n\n\nabs_dif = np.abs(grouped[\"virginica\"] - grouped[\"versicolor\"])\nprint(f'The difference between the means is {abs_dif}')\n\nThe difference between the means is 0.47000000000000064\n\n\n\ndata['species_shuffled'] = data['species'].sample(frac=1, replace=False).values\ndata\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nspecies\nspecies_shuffled\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2.0\nvirginica\nvirginica\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2.0\nvirginica\nversicolor\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2.0\nvirginica\nversicolor\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2.0\nvirginica\nvirginica\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2.0\nvirginica\nversicolor\n\n\n105\n7.6\n3.0\n6.6\n2.1\n2.0\nvirginica\nversicolor\n\n\n106\n4.9\n2.5\n4.5\n1.7\n2.0\nvirginica\nversicolor\n\n\n107\n7.3\n2.9\n6.3\n1.8\n2.0\nvirginica\nversicolor\n\n\n108\n6.7\n2.5\n5.8\n1.8\n2.0\nvirginica\nvirginica\n\n\n109\n7.2\n3.6\n6.1\n2.5\n2.0\nvirginica\nvirginica\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1.0\nversicolor\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1.0\nversicolor\nvirginica\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1.0\nversicolor\nvirginica\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1.0\nversicolor\nversicolor\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1.0\nversicolor\nvirginica\n\n\n55\n5.7\n2.8\n4.5\n1.3\n1.0\nversicolor\nvirginica\n\n\n56\n6.3\n3.3\n4.7\n1.6\n1.0\nversicolor\nversicolor\n\n\n57\n4.9\n2.4\n3.3\n1.0\n1.0\nversicolor\nvirginica\n\n\n58\n6.6\n2.9\n4.6\n1.3\n1.0\nversicolor\nversicolor\n\n\n59\n5.2\n2.7\n3.9\n1.4\n1.0\nversicolor\nvirginica\n\n\n\n\n\n\n\n\ndata['species_shuffled'] = data['species'].sample(frac=1, replace=False).values\ngrouped = data.groupby(\"species_shuffled\", observed=True)['sepal length (cm)'].mean()\nprint(f'The mean of sepal length for virginica is {grouped[\"virginica\"]}')\nprint(f'The mean of sepal length for versicolor is {grouped[\"versicolor\"]}')\n\nsampled_diff = np.abs(grouped[\"virginica\"] - grouped[\"versicolor\"])\nprint(f'The difference between the means is {sampled_diff}')\n\nThe mean of sepal length for virginica is 6.58\nThe mean of sepal length for versicolor is 6.09\nThe difference between the means is 0.4900000000000002\n\n\n\nruns = 1000\ndiffs = np.zeros(runs)\nfor ii in range(runs):\n    data['species_shuffled'] = data['species'].sample(frac=1, replace=False).values\n    grouped = data.groupby(\"species_shuffled\", observed=True)['sepal length (cm)'].mean()\n    diffs[ii]= np.abs(grouped[\"virginica\"] - grouped[\"versicolor\"])\n\n\n# compute our p-value\nlarger = np.where(diffs&gt;=abs_dif, 1, 0)\np_val = np.sum(larger)/runs\nprint(f'The p-value is {p_val}')\nnp.sum(larger)\n\nThe p-value is 0.181\n\n\n181\n\n\n\n6.4.1 Let’s look at it visually\n\nimport seaborn as sns\nax = sns.histplot(diffs, stat='density')\nax.axvline(abs_dif)\n\n&lt;matplotlib.lines.Line2D at 0x128ad8bd0&gt;"
  },
  {
    "objectID": "tirgul/tirgul_7/tirgul_7.html#dependence-between-variables",
    "href": "tirgul/tirgul_7/tirgul_7.html#dependence-between-variables",
    "title": "7  Tutorial 7",
    "section": "7.1 Dependence between variables",
    "text": "7.1 Dependence between variables\n\nCorrelation and regression\nLinear regression – OLS\n\n\n7.1.1 1. Correlation and Regression\n\nPearson’s coefficient measures linear correlation (r).\nSpearman coefficient compares the ‘ranks’ of data.\nNumPy, SciPy, and Pandas all have functions that can be used to calculate these coefficients.\n\n\n\n7.1.2 Correlation in Numpy (Pearson’s r)\nRequired packages\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nLet’s make up some numbers\n\nmu, sigma = 0, 0.1 # mean and standard deviation\nx = np.random.normal(mu, sigma, 50)\ny = np.random.normal(mu, sigma, 50)\nz = np.random.normal(mu, sigma, 50)\n\nWhat does our data look like?\n\nfig, ax = plt.subplots()\nsns.scatterplot(x=x, y=y, ax=ax)\nax.set(xlabel='X', ylabel='Y')\n\n[Text(0.5, 0, 'X'), Text(0, 0.5, 'Y')]\n\n\n\n\n\nUse Numpy to get pearson’s correlation between x and y\n\nnp.corrcoef(x, y)\n\narray([[ 1.        , -0.12455511],\n       [-0.12455511,  1.        ]])\n\n\n\n\n7.1.3 The result is a correlation matrix. Each cell in the table shows the correlation between two variables.\n\nThe values on the main diagonal of the correlation matrix (upper left and lower right) are equal to 1. These corresponds to the correlation coefficient for x and x and y and y, so they will always be equal to 1.\nThe values on the bottom left and top right show the pearson’s correlation coefficient for x and y.\n\nWe can do the same thing with more than two variables\n\nnp.corrcoef([x, y, z])\n\narray([[ 1.        , -0.12455511,  0.03365426],\n       [-0.12455511,  1.        , -0.14554429],\n       [ 0.03365426, -0.14554429,  1.        ]])"
  },
  {
    "objectID": "tirgul/tirgul_7/tirgul_7.html#correlation-in-scipy-pearson-and-spearman",
    "href": "tirgul/tirgul_7/tirgul_7.html#correlation-in-scipy-pearson-and-spearman",
    "title": "7  Tutorial 7",
    "section": "7.2 Correlation in SciPy (Pearson and Spearman)",
    "text": "7.2 Correlation in SciPy (Pearson and Spearman)\nImport required packages\n\nimport scipy.stats\n\n\n# pearson\nscipy.stats.pearsonr(x, y)\n\nPearsonRResult(statistic=-0.12455510674830719, pvalue=0.3887841983709771)\n\n\n\n# spearman\nscipy.stats.spearmanr(x, y)\n\nSignificanceResult(statistic=-0.13661464585834332, pvalue=0.34413653070929573)\n\n\n\n7.2.0.1 Note: these functions return both the correlation coefficient and the p-value"
  },
  {
    "objectID": "tirgul/tirgul_7/tirgul_7.html#correlation-in-pandas",
    "href": "tirgul/tirgul_7/tirgul_7.html#correlation-in-pandas",
    "title": "7  Tutorial 7",
    "section": "7.3 Correlation in Pandas",
    "text": "7.3 Correlation in Pandas\nTime for some real data :)\n\n7.3.1 Today’s dataset: Birthweight\nThis dataset contains information on new born babies and their parents.\nhttps://www.sheffield.ac.uk/mash/statistics/datasets\nRequired packages\n\nimport pandas as pd\n\n\n# import dataset\nfile = \"./birthweight.csv\"\ndf = pd.read_csv(file)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nLength\nBirthweight\nHeadcirc\nGestation\nsmoker\nmage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\n\n\n\n\n0\n56\n4.55\n34\n44\n0\n20\n0\n162\n57\n23\n10\n35\n179\n0\n0\n\n\n1\n53\n4.32\n36\n40\n0\n19\n0\n171\n62\n19\n12\n0\n183\n0\n0\n\n\n2\n58\n4.10\n39\n41\n0\n35\n0\n172\n58\n31\n16\n25\n185\n0\n1\n\n\n3\n53\n4.07\n38\n44\n0\n20\n0\n174\n68\n26\n14\n25\n189\n0\n0\n\n\n4\n54\n3.94\n37\n42\n0\n24\n0\n175\n66\n30\n12\n0\n184\n0\n0\n\n\n\n\n\n\n\n\n\n7.3.2 Is there any correlation between birthweight and head circumfrence?\n\n# First, let's visualize the data\nsns.scatterplot(x=df['Birthweight'], y=df['Headcirc'])\n\n&lt;Axes: xlabel='Birthweight', ylabel='Headcirc'&gt;\n\n\n\n\n\n\ndf['Birthweight'].corr(df['Headcirc'])\n\n0.6846156184774087\n\n\n\n\n7.3.3 There are lots of ways to calculate correlation in Pandas…\n\ndf['Birthweight'].corr(df['Headcirc'],\n                       method='pearson')\n\n0.6846156184774087\n\n\n\ndf['Birthweight'].corr(df['Headcirc'],\n                       method='spearman')\n\n0.6772599775176383\n\n\n\n\n7.3.4 We can also find the correlation between all the variables in our dataframe at once\n\npearsoncorr = df.corr(method ='pearson')\npearsoncorr\n\n\n\n\n\n\n\n\nLength\nBirthweight\nHeadcirc\nGestation\nsmoker\nmage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\n\n\n\n\nLength\n1.000000\n0.726833\n0.563172\n0.705111\n-1.534062e-01\n0.075268\n-0.039843\n0.484992\n3.981974e-01\n0.137184\n0.079485\n0.008800\n0.208358\n-0.609928\n0.130502\n\n\nBirthweight\n0.726833\n1.000000\n0.684616\n0.708303\n-3.142339e-01\n0.000173\n-0.152335\n0.363055\n4.008856e-01\n0.175710\n0.071045\n-0.093136\n0.031022\n-0.651964\n-0.108947\n\n\nHeadcirc\n0.563172\n0.684616\n1.000000\n0.404635\n-1.828719e-01\n0.145842\n-0.132988\n0.337047\n3.028541e-01\n0.301151\n0.123892\n-0.046837\n0.041509\n-0.446849\n0.055386\n\n\nGestation\n0.705111\n0.708303\n0.404635\n1.000000\n-9.474608e-02\n0.010778\n0.043195\n0.210503\n2.550824e-01\n0.142175\n0.130987\n-0.113831\n0.207597\n-0.602935\n0.007395\n\n\nsmoker\n-0.153406\n-0.314234\n-0.182872\n-0.094746\n1.000000e+00\n0.212479\n0.727218\n0.000353\n9.808342e-16\n0.197501\n-0.014891\n0.417633\n0.110633\n0.253012\n0.146938\n\n\nmage\n0.075268\n0.000173\n0.145842\n0.010778\n2.124788e-01\n1.000000\n0.340294\n0.059956\n2.741677e-01\n0.806584\n0.441683\n0.090927\n-0.199547\n-0.076394\n0.692664\n\n\nmnocig\n-0.039843\n-0.152335\n-0.132988\n0.043195\n7.272181e-01\n0.340294\n1.000000\n0.126439\n1.489446e-01\n0.248425\n0.198526\n0.257307\n0.020672\n0.035384\n0.290574\n\n\nmheight\n0.484992\n0.363055\n0.337047\n0.210503\n3.532676e-04\n0.059956\n0.126439\n1.000000\n6.806217e-01\n-0.079870\n0.035297\n0.048398\n0.274338\n-0.198151\n0.116002\n\n\nmppwt\n0.398197\n0.400886\n0.302854\n0.255082\n9.808342e-16\n0.274168\n0.148945\n0.680622\n1.000000e+00\n0.255706\n0.180374\n0.057163\n0.092983\n-0.353974\n0.136853\n\n\nfage\n0.137184\n0.175710\n0.301151\n0.142175\n1.975014e-01\n0.806584\n0.248425\n-0.079870\n2.557058e-01\n1.000000\n0.300471\n0.135862\n-0.269377\n-0.245095\n0.351405\n\n\nfedyrs\n0.079485\n0.071045\n0.123892\n0.130987\n-1.489058e-02\n0.441683\n0.198526\n0.035297\n1.803741e-01\n0.300471\n1.000000\n-0.263103\n0.017798\n-0.191273\n0.278682\n\n\nfnocig\n0.008800\n-0.093136\n-0.046837\n-0.113831\n4.176330e-01\n0.090927\n0.257307\n0.048398\n5.716254e-02\n0.135862\n-0.263103\n1.000000\n0.329364\n0.266013\n-0.088989\n\n\nfheight\n0.208358\n0.031022\n0.041509\n0.207597\n1.106327e-01\n-0.199547\n0.020672\n0.274338\n9.298347e-02\n-0.269377\n0.017798\n0.329364\n1.000000\n0.098688\n-0.188230\n\n\nlowbwt\n-0.609928\n-0.651964\n-0.446849\n-0.602935\n2.530122e-01\n-0.076394\n0.035384\n-0.198151\n-3.539738e-01\n-0.245095\n-0.191273\n0.266013\n0.098688\n1.000000\n0.099340\n\n\nmage35\n0.130502\n-0.108947\n0.055386\n0.007395\n1.469385e-01\n0.692664\n0.290574\n0.116002\n1.368534e-01\n0.351405\n0.278682\n-0.088989\n-0.188230\n0.099340\n1.000000\n\n\n\n\n\n\n\n\n\n7.3.5 That isn’t so helpful… we can also make a heatmap to display the data visually\n\nfig, ax = plt.subplots()\n\nsns.heatmap(pearsoncorr,\n           cmap='RdBu_r', ax=ax, vmin=-1, vmax=1)\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "tirgul/tirgul_7/tirgul_7.html#linear-regression",
    "href": "tirgul/tirgul_7/tirgul_7.html#linear-regression",
    "title": "7  Tutorial 7",
    "section": "7.4 Linear Regression",
    "text": "7.4 Linear Regression\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables).\nhttps://en.wikipedia.org/wiki/Linear_regression\nWe have a lot of options for for studying linear regression\n\n7.4.1 Scipy.stats\n\nresult = scipy.stats.linregress(df['Birthweight'],\n                                df['Headcirc'])\n\n\nprint(result)\n\nLinregressResult(slope=2.720563928236349, intercept=25.58239845298082, rvalue=0.6846156184774087, pvalue=5.734797978444235e-07, stderr=0.45798646115832675, intercept_stderr=1.5416553949208147)\n\n\nA reminder of what the data look like…\n\nfig, ax = plt.subplots()\nsns.scatterplot(x=df['Birthweight'], y=df['Headcirc'], ax=ax)\nx = np.linspace(df['Birthweight'].min(), df['Birthweight'].max(), 100)\ny = result.intercept + result.slope * x\nax.plot(x, y, color='red')\n\n\n\n\n\n\n\n\n\nresult.slope\n\n2.720563928236349\n\n\n\nresult.intercept\n\n25.58239845298082\n\n\n\nresult.rvalue**2\n\n0.46869854506320485\n\n\n\n\n7.4.2 Using sklearn\n\nfrom sklearn import linear_model\n\n\nmy_model = linear_model.LinearRegression()\nresults = my_model.fit(df[['Birthweight']], df[['Headcirc']])\nprint(\"The linear model is: Y = {:.5} + {:.5}X\".format(results.intercept_[0],\n                                                       results.coef_[0][0]))\n\nThe linear model is: Y = 25.582 + 2.7206X\n\n\n\n# the results are the same, but the sklearn package has some other features\npredictions = results.predict(df[['Birthweight']])\n\n\nfig, ax = plt.subplots()\n# we can add the predicted values to the original plot\nax = sns.scatterplot(x=df['Birthweight'], \n                     y=df['Headcirc'],\n                     color=\"0.0\")\nax.plot(df['Birthweight'],\n        predictions,\n        color=\"b\")"
  },
  {
    "objectID": "tirgul/tirgul_7/tirgul_7.html#hypothesis-testing",
    "href": "tirgul/tirgul_7/tirgul_7.html#hypothesis-testing",
    "title": "7  Tutorial 7",
    "section": "7.5 Hypothesis testing",
    "text": "7.5 Hypothesis testing\n\nNull hypothesis 1: The actual intercept is equal to zero\nNull hypothesis 2: The actual slope is equal to zero\n\nUsing the statsmodels.api package\n\nimport statsmodels.api as sm\n\nSet up the model\n\nX = sm.add_constant(df['Birthweight'])\nY = df['Headcirc']\nmodel = sm.OLS(Y, X)  # OLS = ordinary least squares\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Headcirc   R-squared:                       0.469\nModel:                            OLS   Adj. R-squared:                  0.455\nMethod:                 Least Squares   F-statistic:                     35.29\nDate:                Tue, 16 Jul 2024   Prob (F-statistic):           5.73e-07\nTime:                        09:12:12   Log-Likelihood:                -82.574\nNo. Observations:                  42   AIC:                             169.1\nDf Residuals:                      40   BIC:                             172.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          25.5824      1.542     16.594      0.000      22.467      28.698\nBirthweight     2.7206      0.458      5.940      0.000       1.795       3.646\n==============================================================================\nOmnibus:                        1.089   Durbin-Watson:                   2.132\nProb(Omnibus):                  0.580   Jarque-Bera (JB):                1.007\nSkew:                          -0.193   Prob(JB):                        0.604\nKurtosis:                       2.347   Cond. No.                         20.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n7.5.0.1 The P-value is the answer to the question “how likely is it that we’d get a test statistic t* as extreme as we did if the null hypothesis were true?\nDoes this output correspond to a one-tailed or two-tailed test?\nIf we want to test whether the slope is different from 0, we need a two-sided test.\nIf we want to test a specific direction, we can use a one-sided test. To do this, we need to divide the p-value in the table above in half.\nRead more: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/\n\nprint(f'p value of the intercept: {results.pvalues.iloc[0]}, p value of the slope: {results.pvalues.iloc[1]}')\n\np value of the intercept: 1.5527553380020346e-19, p value of the slope: 5.734797978444454e-07\n\n\n\n\n7.5.0.2 Can we compute the P-value using permutations?…. YES!"
  },
  {
    "objectID": "tirgul/tirgul_6/group_by.html#multi-index",
    "href": "tirgul/tirgul_6/group_by.html#multi-index",
    "title": "8  Groupby",
    "section": "8.1 multi index",
    "text": "8.1 multi index\nHere’s an example of grouping jointly on two columns, which finds the count of Congressional members broken out by state and then by gender:\n\ndf.groupby([\"state\", \"gender\"])[\"last_name\"].count()\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39943/503199787.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"state\", \"gender\"])[\"last_name\"].count()\n\n\nstate  gender\nAK     F           0\n       M          16\nAL     F           4\n       M         205\nAR     F           5\n                ... \nWI     M         198\nWV     F           1\n       M         119\nWY     F           1\n       M          39\nName: last_name, Length: 116, dtype: int64\n\n\nNote: When we use groupby on more than one column, we create a MultiIndex\n\nn_by_state_gender = df.groupby([\"state\", \"gender\"])[\"last_name\"].count()\ntype(n_by_state_gender)\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39943/197250217.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  n_by_state_gender = df.groupby([\"state\", \"gender\"])[\"last_name\"].count()\n\n\npandas.core.series.Series\n\n\n\nn_by_state_gender.index\n\nMultiIndex([('AK', 'F'),\n            ('AK', 'M'),\n            ('AL', 'F'),\n            ('AL', 'M'),\n            ('AR', 'F'),\n            ('AR', 'M'),\n            ('AS', 'F'),\n            ('AS', 'M'),\n            ('AZ', 'F'),\n            ('AZ', 'M'),\n            ...\n            ('VT', 'F'),\n            ('VT', 'M'),\n            ('WA', 'F'),\n            ('WA', 'M'),\n            ('WI', 'F'),\n            ('WI', 'M'),\n            ('WV', 'F'),\n            ('WV', 'M'),\n            ('WY', 'F'),\n            ('WY', 'M')],\n           names=['state', 'gender'], length=116)\n\n\nIf we want to keep the data in columns, we can use the as_index parameter\n\ndf.groupby([\"state\", \"gender\"], as_index=False)[\"last_name\"].count()\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39943/1045572165.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby([\"state\", \"gender\"], as_index=False)[\"last_name\"].count()\n\n\n\n\n\n\n\n\n\nstate\ngender\nlast_name\n\n\n\n\n0\nAK\nF\n0\n\n\n1\nAK\nM\n16\n\n\n2\nAL\nF\n4\n\n\n3\nAL\nM\n205\n\n\n4\nAR\nF\n5\n\n\n...\n...\n...\n...\n\n\n111\nWI\nM\n198\n\n\n112\nWV\nF\n1\n\n\n113\nWV\nM\n119\n\n\n114\nWY\nF\n1\n\n\n115\nWY\nM\n39\n\n\n\n\n116 rows × 3 columns\n\n\n\nYou might have noticed that groupby automatically sorts the data. We can also change this, if we want.\n\ndf.groupby(\"state\", sort=False)[\"last_name\"].count()\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39943/966111394.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby(\"state\", sort=False)[\"last_name\"].count()\n\n\nstate\nDE      97\nVA     433\nSC     251\nMD     305\nPA    1053\nMA     427\nNJ     359\nGA     317\nNY    1467\nNC     356\nCT     240\nVT     115\nKY     373\nRI     107\nNH     181\nTN     301\nOH     675\nMS     155\nOL       2\nIN     343\nLA     199\nIL     488\nMO     334\nAL     209\nAR     117\nME     175\nFL     161\nMI     296\nIA     205\nWI     198\nTX     263\nCA     367\nOR      90\nMN     161\nNM      57\nNE     127\nWA      96\nKS     143\nUT      55\nNV      56\nCO      92\nWV     120\nDK       9\nAZ      49\nID      59\nMT      53\nWY      40\nDC       2\nND      44\nSD      51\nOK      93\nHI      24\nPR      19\nAK      16\nPI      13\nVI       4\nGU       4\nAS       2\nName: last_name, dtype: int64"
  },
  {
    "objectID": "tirgul/tirgul_6/group_by.html#what-is-actually-happening-here",
    "href": "tirgul/tirgul_6/group_by.html#what-is-actually-happening-here",
    "title": "8  Groupby",
    "section": "8.2 What is actually happening here?",
    "text": "8.2 What is actually happening here?\n\nby_state = df.groupby(\"state\")\nby_state\n\n/var/folders/wn/2bz1970d2w5182zy7h96yfcc0000gn/T/ipykernel_39943/3630249550.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  by_state = df.groupby(\"state\")\n\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x12f9b0890&gt;\n\n\n\nfor state, frame in by_state:\n    print(f\"First 2 entries for {state!r}\")\n    print(\"------------------------\")\n    print(frame.head(2), end=\"\\n\\n\")\n\nFirst 2 entries for 'AK'\n------------------------\n     last_name first_name   birthday gender type state        party\n6617    Waskey      Frank 1875-04-20      M  rep    AK     Democrat\n6645      Cale     Thomas 1848-09-17      M  rep    AK  Independent\n\nFirst 2 entries for 'AL'\n------------------------\n    last_name first_name   birthday gender type state       party\n911   Crowell       John 1780-09-18      M  rep    AL  Republican\n990    Walker       John 1783-08-12      M  sen    AL  Republican\n\nFirst 2 entries for 'AR'\n------------------------\n     last_name first_name   birthday gender type state party\n1000     Bates      James 1788-08-25      M  rep    AR   NaN\n1278    Conway      Henry 1793-03-18      M  rep    AR   NaN\n\nFirst 2 entries for 'AS'\n------------------------\n          last_name first_name   birthday gender type state     party\n10795         Sunia       Fofó 1937-03-13      M  rep    AS  Democrat\n11752  Faleomavaega        Eni 1943-08-15      M  rep    AS  Democrat\n\nFirst 2 entries for 'AZ'\n------------------------\n     last_name first_name   birthday gender type state       party\n3672    Poston    Charles 1825-04-20      M  rep    AZ  Republican\n3723   Goodwin       John 1824-10-18      M  rep    AZ  Republican\n\nFirst 2 entries for 'CA'\n------------------------\n     last_name first_name   birthday gender type state        party\n2648   Gilbert     Edward        NaT      M  rep    CA     Democrat\n2740    Wright     George 1816-06-04      M  rep    CA  Independent\n\nFirst 2 entries for 'CO'\n------------------------\n     last_name first_name   birthday gender type state  \\\n3612    Bennet      Hiram 1826-09-02      M  rep    CO   \n3931  Bradford      Allen 1815-07-23      M  rep    CO   \n\n                        party  \n3612  Conservative Republican  \n3931               Republican  \n\nFirst 2 entries for 'CT'\n------------------------\n     last_name first_name   birthday gender type state               party\n14  Huntington   Benjamin 1736-04-19      M  rep    CT                 NaN\n16     Johnson    William 1727-10-07      M  sen    CT  Pro-Administration\n\nFirst 2 entries for 'DC'\n------------------------\n      last_name first_name   birthday gender type state       party\n4230    Chipman     Norton 1834-03-07      M  rep    DC  Republican\n10819  Fauntroy     Walter 1933-02-06      M  rep    DC    Democrat\n\nFirst 2 entries for 'DE'\n------------------------\n   last_name first_name   birthday gender type state                party\n0    Bassett    Richard 1745-04-02      M  sen    DE  Anti-Administration\n40      Read     George 1733-09-18      M  sen    DE                  NaN\n\nFirst 2 entries for 'DK'\n------------------------\n     last_name first_name   birthday gender type state     party\n3640     Jayne    William 1826-10-08      M  rep    DK       NaN\n3685      Todd       John 1814-04-04      M  rep    DK  Democrat\n\nFirst 2 entries for 'FL'\n------------------------\n      last_name first_name   birthday gender type state party\n1030  Hernández     Joseph 1788-05-26      M  rep    FL   NaN\n1089       Call    Richard 1792-10-24      M  rep    FL   NaN\n\nFirst 2 entries for 'GA'\n------------------------\n   last_name first_name   birthday gender type state                party\n8        Few    William 1748-06-08      M  sen    GA  Anti-Administration\n20   Mathews     George 1739-08-30      M  rep    GA                  NaN\n\nFirst 2 entries for 'GU'\n------------------------\n      last_name first_name   birthday gender type state       party\n10702   Won Pat    Antonio 1908-12-10      M  rep    GU    Democrat\n10872      Blaz        Ben 1928-02-14      M  rep    GU  Republican\n\nFirst 2 entries for 'HI'\n------------------------\n     last_name first_name   birthday gender type state       party\n6393    Wilcox     Robert 1855-02-15      M  rep    HI         NaN\n7522   Baldwin      Henry 1871-01-12      M  rep    HI  Republican\n\nFirst 2 entries for 'IA'\n------------------------\n     last_name first_name   birthday gender type state     party\n1930   Chapman    William 1808-08-11      M  rep    IA  Democrat\n2390  Hastings   Serranus 1813-11-22      M  rep    IA  Democrat\n\nFirst 2 entries for 'ID'\n------------------------\n     last_name first_name   birthday gender type state       party\n3691   Wallace    William 1811-07-19      M  rep    ID  Republican\n3844  Holbrook     Edward 1836-05-06      M  rep    ID    Democrat\n\nFirst 2 entries for 'IL'\n------------------------\n      last_name first_name   birthday gender type state party\n595        Bond   Shadrack 1773-11-24      M  rep    IL   NaN\n772  Stephenson   Benjamin        NaT      M  rep    IL   NaN\n\nFirst 2 entries for 'IN'\n------------------------\n    last_name first_name   birthday gender type state       party\n431     Parke   Benjamin 1777-09-22      M  rep    IN         NaN\n984    Taylor     Waller        NaT      M  sen    IN  Republican\n\nFirst 2 entries for 'KS'\n------------------------\n      last_name first_name   birthday gender type state       party\n3161  Whitfield       John 1818-03-11      M  rep    KS    Democrat\n3420    Parrott     Marcus 1828-10-27      M  rep    KS  Republican\n\nFirst 2 entries for 'KY'\n------------------------\n    last_name   first_name birthday gender type state                party\n55    Edwards         John      NaT      M  sen    KY  Anti-Administration\n113   Greenup  Christopher      NaT      M  rep    KY           Republican\n\nFirst 2 entries for 'LA'\n------------------------\n     last_name first_name birthday gender type state       party\n584  Destréhan       Jean      NaT      M  sen    LA     Unknown\n585   Magruder      Allan      NaT      M  sen    LA  Republican\n\nFirst 2 entries for 'MA'\n------------------------\n   last_name first_name   birthday gender type state               party\n6     Dalton   Tristram 1738-05-28      M  sen    MA  Pro-Administration\n12     Grout   Jonathan 1737-07-23      M  rep    MA                 NaN\n\nFirst 2 entries for 'MD'\n------------------------\n  last_name first_name   birthday gender type state party\n3   Carroll     Daniel 1730-07-22      M  rep    MD   NaN\n5    Contee   Benjamin        NaT      M  rep    MD   NaN\n\nFirst 2 entries for 'ME'\n------------------------\n     last_name first_name   birthday gender type state       party\n1018      Dane     Joseph 1778-10-25      M  rep    ME  Federalist\n1028    Harris       Mark 1779-01-27      M  rep    ME  Republican\n\nFirst 2 entries for 'MI'\n------------------------\n     last_name first_name   birthday gender type state party\n1061    Sibley    Solomon 1769-10-07      M  rep    MI   NaN\n1132   Richard    Gabriel 1767-10-15      M  rep    MI   NaN\n\nFirst 2 entries for 'MN'\n------------------------\n      last_name first_name   birthday gender type state     party\n2864     Sibley      Henry 1811-02-20      M  rep    MN       NaN\n3239  Kingsbury    William 1828-06-04      M  rep    MN  Democrat\n\nFirst 2 entries for 'MO'\n------------------------\n     last_name first_name   birthday gender type state party\n627  Hempstead     Edward 1780-06-03      M  rep    MO   NaN\n712     Easton      Rufus 1774-05-04      M  rep    MO   NaN\n\nFirst 2 entries for 'MS'\n------------------------\n    last_name  first_name   birthday gender type state party\n256    Greene      Thomas 1758-02-26      M  rep    MS   NaN\n260    Hunter  Narsworthy        NaT      M  rep    MS   NaN\n\nFirst 2 entries for 'MT'\n------------------------\n      last_name first_name   birthday gender type state     party\n3754     McLean     Samuel 1826-08-07      M  rep    MT  Democrat\n3939  Cavanaugh      James 1823-07-04      M  rep    MT  Democrat\n\nFirst 2 entries for 'NC'\n------------------------\n   last_name first_name   birthday gender type state               party\n13   Hawkins   Benjamin 1754-08-15      M  sen    NC  Pro-Administration\n17  Johnston     Samuel 1733-12-15      M  sen    NC  Pro-Administration\n\nFirst 2 entries for 'ND'\n------------------------\n     last_name first_name   birthday gender type state       party\n5382     Casey      Lyman 1837-05-06      M  sen    ND  Republican\n5477    Pierce    Gilbert 1839-01-11      M  sen    ND  Republican\n\nFirst 2 entries for 'NE'\n------------------------\n     last_name first_name   birthday gender type state     party\n2952  Giddings   Napoleon 1816-01-02      M  rep    NE  Democrat\n3066   Chapman       Bird 1821-08-24      M  rep    NE  Democrat\n\nFirst 2 entries for 'NH'\n------------------------\n    last_name first_name   birthday gender type state       party\n99    Wingate      Paine 1739-05-14      M  rep    NH         NaN\n120   Langdon       John 1741-06-26      M  sen    NH  Republican\n\nFirst 2 entries for 'NJ'\n------------------------\n   last_name first_name   birthday gender type state               party\n7      Elmer   Jonathan 1745-11-29      M  sen    NJ  Pro-Administration\n23  Paterson    William 1745-12-24      M  sen    NJ  Pro-Administration\n\nFirst 2 entries for 'NM'\n------------------------\n      last_name first_name   birthday gender type state     party\n2883  Weightman    Richard 1816-12-28      M  rep    NM  Democrat\n3418      Otero     Miguel 1829-06-21      M  rep    NM       NaN\n\nFirst 2 entries for 'NV'\n------------------------\n        last_name first_name   birthday gender type state       party\n3508  Cradlebaugh       John 1819-02-22      M  rep    NV         NaN\n3662         Mott     Gordon 1812-10-21      M  rep    NV  Republican\n\nFirst 2 entries for 'NY'\n------------------------\n         last_name first_name   birthday gender type state party\n9            Floyd    William 1734-12-17      M  rep    NY   NaN\n26  Van Rensselaer   Jeremiah 1738-08-27      M  rep    NY   NaN\n\nFirst 2 entries for 'OH'\n------------------------\n    last_name first_name   birthday gender type state       party\n226  McMillan    William 1764-03-02      M  rep    OH         NaN\n254   Fearing       Paul 1762-02-28      M  rep    OH  Federalist\n\nFirst 2 entries for 'OK'\n------------------------\n     last_name first_name   birthday gender type state        party\n5599    Harvey      David 1845-03-20      M  rep    OK   Republican\n6058  Callahan      James 1852-12-19      M  rep    OK  Free Silver\n\nFirst 2 entries for 'OL'\n------------------------\n    last_name first_name   birthday gender type state party\n404     Clark     Daniel        NaT      M  rep    OL   NaN\n503   Poydras     Julien 1740-04-03      M  rep    OL   NaN\n\nFirst 2 entries for 'OR'\n------------------------\n     last_name first_name   birthday gender type state     party\n2726  Thurston     Samuel 1816-04-15      M  rep    OR  Democrat\n3396      Lane     Joseph 1801-12-14      M  sen    OR  Democrat\n\nFirst 2 entries for 'PA'\n------------------------\n   last_name first_name   birthday gender type state                party\n4     Clymer     George 1739-03-16      M  rep    PA                  NaN\n19    Maclay    William 1737-07-20      M  sen    PA  Anti-Administration\n\nFirst 2 entries for 'PI'\n------------------------\n             last_name first_name   birthday gender type state party\n6833            Ocampo      Pablo 1853-01-25      M  rep    PI   NaN\n6937  Legarda Y Tuason     Benito 1853-09-27      M  rep    PI   NaN\n\nFirst 2 entries for 'PR'\n------------------------\n      last_name first_name   birthday gender type state       party\n6424    Degetau   Federico 1862-12-05      M  rep    PR  Republican\n6809  Larrinaga      Tulio 1847-01-15      M  rep    PR    Unionist\n\nFirst 2 entries for 'RI'\n------------------------\n    last_name first_name   birthday gender type state       party\n61   Bradford    William 1729-11-04      M  sen    RI  Federalist\n105    Bourne   Benjamin 1755-09-09      M  rep    RI  Federalist\n\nFirst 2 entries for 'SC'\n------------------------\n   last_name first_name   birthday gender type state               party\n2      Burke    Aedanus 1743-06-16      M  rep    SC                 NaN\n15     Izard      Ralph        NaT      M  sen    SC  Pro-Administration\n\nFirst 2 entries for 'SD'\n------------------------\n     last_name first_name   birthday gender type state       party\n5421   Gifford      Oscar 1842-10-20      M  rep    SD  Republican\n5460     Moody     Gideon 1832-10-16      M  sen    SD  Republican\n\nFirst 2 entries for 'TN'\n------------------------\n    last_name first_name   birthday gender type state       party\n141     White      James 1749-06-16      M  rep    TN         NaN\n142    Blount    William 1749-03-26      M  sen    TN  Republican\n\nFirst 2 entries for 'TX'\n------------------------\n     last_name first_name   birthday gender type state     party\n2567  Pilsbury    Timothy 1789-04-12      M  rep    TX  Democrat\n2669   Kaufman      David 1813-12-18      M  rep    TX  Democrat\n\nFirst 2 entries for 'UT'\n------------------------\n      last_name first_name   birthday gender type state     party\n3482  Bernhisel       John 1799-07-23      M  rep    UT      Whig\n3645     Kinney       John 1816-04-02      M  rep    UT  Democrat\n\nFirst 2 entries for 'VA'\n------------------------\n   last_name  first_name   birthday gender type state                party\n1      Bland  Theodorick 1742-03-21      M  rep    VA                  NaN\n11   Grayson     William        NaT      M  sen    VA  Anti-Administration\n\nFirst 2 entries for 'VI'\n------------------------\n      last_name first_name   birthday gender type state       party\n10494     Evans     Melvin 1917-08-07      M  rep    VI  Republican\n11086   de Lugo        Ron 1930-08-02      M  rep    VI    Democrat\n\nFirst 2 entries for 'VT'\n------------------------\n   last_name first_name   birthday gender type state                party\n41  Robinson      Moses 1741-03-22      M  sen    VT  Anti-Administration\n86     Niles  Nathaniel 1741-04-03      M  rep    VT                  NaN\n\nFirst 2 entries for 'WA'\n------------------------\n      last_name first_name   birthday gender type state     party\n2977  Lancaster   Columbia 1803-08-26      M  rep    WA  Democrat\n3050   Anderson      James 1822-02-16      M  rep    WA  Democrat\n\nFirst 2 entries for 'WI'\n------------------------\n     last_name first_name   birthday gender type state     party\n2409    Martin     Morgan 1805-03-31      M  rep    WI  Democrat\n2502   Darling      Mason 1801-05-18      M  rep    WI  Democrat\n\nFirst 2 entries for 'WV'\n------------------------\n       last_name first_name   birthday gender type state  \\\n3613       Blair      Jacob 1821-04-11      M  rep    WV   \n3688  Van Winkle      Peter 1808-09-07      M  sen    WV   \n\n                       party  \n3613  Unconditional Unionist  \n3688              Republican  \n\nFirst 2 entries for 'WY'\n------------------------\n     last_name first_name   birthday gender type state       party\n4007  Nuckolls    Stephen 1825-08-16      M  rep    WY    Democrat\n4136     Jones    William 1842-02-20      M  rep    WY  Republican\n\n\n\nWhen you use groupby, you automatically create a dictionary with the different group labels.\n\nby_state.groups[\"PA\"]\n\nIndex([    4,    19,    21,    27,    38,    57,    69,    76,    84,    88,\n       ...\n       11838, 11862, 11871, 11873, 11883, 11887, 11926, 11938, 11952, 11965],\n      dtype='int64', length=1053)"
  },
  {
    "objectID": "tirgul/tirgul_6/group_by.html#get-group",
    "href": "tirgul/tirgul_6/group_by.html#get-group",
    "title": "8  Groupby",
    "section": "8.3 get group",
    "text": "8.3 get group\nYou can also use .get_group() as a way to drill down to the sub-table from a single group:\n\nby_state.get_group(\"PA\")\n\n\n\n\n\n\n\n\nlast_name\nfirst_name\nbirthday\ngender\ntype\nstate\nparty\n\n\n\n\n4\nClymer\nGeorge\n1739-03-16\nM\nrep\nPA\nNaN\n\n\n19\nMaclay\nWilliam\n1737-07-20\nM\nsen\nPA\nAnti-Administration\n\n\n21\nMorris\nRobert\n1734-01-20\nM\nsen\nPA\nPro-Administration\n\n\n27\nWynkoop\nHenry\n1737-03-02\nM\nrep\nPA\nNaN\n\n\n38\nJacobs\nIsrael\n1726-06-09\nM\nrep\nPA\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11887\nBrady\nRobert\n1945-04-07\nM\nrep\nPA\nDemocrat\n\n\n11926\nShuster\nBill\n1961-01-10\nM\nrep\nPA\nRepublican\n\n\n11938\nRothfus\nKeith\n1962-04-25\nM\nrep\nPA\nRepublican\n\n\n11952\nCostello\nRyan\n1976-09-07\nM\nrep\nPA\nRepublican\n\n\n11965\nMarino\nTom\n1952-08-15\nM\nrep\nPA\nRepublican\n\n\n\n\n1053 rows × 7 columns\n\n\n\nThis is virtually equivalent to using .loc[]. You could get the same output with something like df.loc[df[\"state\"] == \"PA\"]."
  },
  {
    "objectID": "tirgul/tirgul_8/bonferroni.html#bonferroni-correction-for-multiple-hypotheses",
    "href": "tirgul/tirgul_8/bonferroni.html#bonferroni-correction-for-multiple-hypotheses",
    "title": "9  Multiple hypothesis correction",
    "section": "9.1 Bonferroni correction for multiple hypotheses",
    "text": "9.1 Bonferroni correction for multiple hypotheses\nThe Bonferroni correction is a multiple-comparison correction used when several dependent or independent statistical tests are being performed simultaneously (since while a given alpha value alpha may be appropriate for each individual comparison, it is not for the set of all comparisons). In order to avoid a lot of spurious positives, the alpha value needs to be lowered to account for the number of comparisons being performed.\nIn multiple hypothesis testing there are two kinds of errors that must be to considered: 1. Type I error: The rejection of a true null hypothesis (also known as a “false positive” finding or conclusion; example: “an innocent person is convicted”) 2. Type II error (False negative): The non-rejection of a false null hypothesis (also known as a “false negative” finding or conclusion; example: “a guilty person is not convicted”\nhttps://mathworld.wolfram.com/BonferroniCorrection.html\n\n9.1.1 Example from gene editing\nBackground: A researcher analyzes thousands of genese to identify “differentially expressed genes” between two groups (e.g., normal vs. treated), which could alter biological mechanisms of a species in response to a particular treatment.\nIf we analyze 10,000 results with a signifance level of 0.05, then we should expect hundreds of false positives.\nTo control false discoveries from multiple hypothesis testing, it is imperative to adjust the significance level (α) to reduce the probability of getting Type I error.\nhttps://www.reneshbedre.com/blog/multiple-hypothesis-testing-corrections.html\nImport packages\n\nimport numpy as np\nfrom statsmodels.stats.multitest import multipletests\n\nGenerate 500 random p-values\n\nrand_num = np.random.random(500)\n\nSet alpha value to 0.05\n\n# alpha value\nalpha = 0.05\n\nWhat would happen if we didn’t correct alpha value?\n\n# without correction, how many times would we reject the null hypothesis?\nprint(len(rand_num[np.where(rand_num&lt;alpha)]))\n\n25\n\n\nWith Bonferroni correction\n\np_adjusted = multipletests(pvals=rand_num, alpha=alpha, method='bonferroni')\nprint(len(p_adjusted[1][np.where(p_adjusted[1]&lt;alpha)]))\n\n0"
  },
  {
    "objectID": "tirgul/tirgul_8/bonferroni.html#benjamini-hochberg-correction-for-multiple-hypotheses",
    "href": "tirgul/tirgul_8/bonferroni.html#benjamini-hochberg-correction-for-multiple-hypotheses",
    "title": "9  Multiple hypothesis correction",
    "section": "9.2 Benjamini-Hochberg correction for multiple hypotheses",
    "text": "9.2 Benjamini-Hochberg correction for multiple hypotheses\n\np_adjusted = multipletests(pvals=rand_num, alpha=alpha, method='fdr_bh')\nprint(len(p_adjusted[1][np.where(p_adjusted[1]&lt;alpha)]))\n\n0"
  },
  {
    "objectID": "tirgul/tirgul_8/logistic_regression.html#whats-the-difference-between-linear-regression-and-logistic-regression",
    "href": "tirgul/tirgul_8/logistic_regression.html#whats-the-difference-between-linear-regression-and-logistic-regression",
    "title": "10  Logistic regression",
    "section": "10.1 What’s the difference between linear regression and logistic regression?",
    "text": "10.1 What’s the difference between linear regression and logistic regression?\nLogistic regression analysis is used to examine the association of (categorical or continuous) independent variable(s) with one dichotomous dependent variable. This is in contrast to linear regression analysis in which the dependent variable is a continuous variable.\nhttps://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning"
  },
  {
    "objectID": "tirgul/tirgul_8/logistic_regression.html#logistic-regression-in-python",
    "href": "tirgul/tirgul_8/logistic_regression.html#logistic-regression-in-python",
    "title": "10  Logistic regression",
    "section": "10.2 Logistic regression in Python",
    "text": "10.2 Logistic regression in Python\nhttps://realpython.com/logistic-regression-python/\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
  },
  {
    "objectID": "tirgul/tirgul_8/logistic_regression.html#logistic-regression-simple-example",
    "href": "tirgul/tirgul_8/logistic_regression.html#logistic-regression-simple-example",
    "title": "10  Logistic regression",
    "section": "10.3 Logistic regression: simple example",
    "text": "10.3 Logistic regression: simple example\nImport packages\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nMake up data\n\nx = np.arange(10).reshape(-1, 1)\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n\n\nx\n\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8],\n       [9]])\n\n\nNote: we use reshape on x because when using the LogisticRegression function the x array must be two-dimensional.\nUsing reshape() with the arguments -1, 1 gives us as many rows as needed and one column.\n\nfig, ax = plt.subplots()\nax.scatter(x, y, \n           color='black', \n           s=100, \n           label=\"actual data\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x128e72bd0&gt;\n\n\n\n\n\nCreate the model\n\nmodel = LogisticRegression(solver='liblinear')\n\nTrain the model\n\nmodel.fit(x, y)\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(solver='liblinear')\n\n\nAlternatively, we can create and fit the model in just one step\n\n\nmodel = LogisticRegression(solver='liblinear', random_state=0).fit(x, y)\n\nOur potential y-values… not very surprising\n\nmodel.classes_\n\narray([0, 1])\n\n\nThe model’s intercept\n\nmodel.intercept_\n\narray([-1.04608067])\n\n\nThe model’s coefficient\n\nmodel.coef_\n\narray([[0.51491375]])\n\n\nEvaluate the model\n\nmodel.predict_proba(x)\n\narray([[0.74002157, 0.25997843],\n       [0.62975524, 0.37024476],\n       [0.5040632 , 0.4959368 ],\n       [0.37785549, 0.62214451],\n       [0.26628093, 0.73371907],\n       [0.17821501, 0.82178499],\n       [0.11472079, 0.88527921],\n       [0.07186982, 0.92813018],\n       [0.04422513, 0.95577487],\n       [0.02690569, 0.97309431]])\n\n\nThis returns the matrix of probabilities that the predicted output is equal to zero or one. The first column is the probability of the predicted output being zero, that is 1 - 𝑝(𝑥). The second column is the probability that the output is one, or 𝑝(𝑥).\nYou can get the actual predictions, based on the probability matrix and the values of 𝑝(𝑥), with .predict(). This function returns the predicted output values as a one-dimensional array.\n\nmodel.predict(x)\n\narray([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n\n\nPlot the results\n\nfig, ax = plt.subplots()\nax.scatter(x, y,\n           color='black', \n           s=100, \n           label=\"actual data\")\nax.scatter(x, model.predict(x), \n           color='red', \n           label=\"predicted\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x12b0c6010&gt;\n\n\n\n\n\n\nx_test = np.linspace(0, 10, 300).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(x, y, \n           color='black', \n           s=100, \n           label=\"actual data\")\nax.scatter(x_test, \n           model.predict(x_test), \n           color='red', \n           label=\"predicted\")\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x12b0b7c10&gt;\n\n\n\n\n\nAccording to the logistic function:\n### p=\\frac{1}{1+e^{-(a+bx)}} * expit() This is a function from the SciPy library, specifically from the scipy.special module. It calculates the sigmoid function, which is defined as 1 / (1 + exp(-x)).\n* .ravel() This function is called on the result of expit() to flatten or reshape the output as a 1-dimensional array. It converts a potentially multi-dimensional array into a contiguous flattened array.\n\nsigmoid = expit(x_test * model.coef_ + model.intercept_).ravel()\n\nfig, ax = plt.subplots()\nax.scatter(x, y, \n           color='black', \n           s=100, \n           label=\"actual data\")\nax.scatter(x_test, \n           model.predict(x_test), \n           color='red', \n           label=\"predicted\")\nax.plot(x_test, \n        sigmoid, \n        color='green', \n        linewidth=3, \n        label='logistic sigmoid')\nax.axhline(y=0.5, \n           color='black', \n           ls='--', \n           label='y = 0.5')\nax.legend()\n\n&lt;matplotlib.legend.Legend at 0x12b192990&gt;\n\n\n\n\n\nGet the model score\n\n\nmodel.score(x, y)\n\n0.9\n\n\n\n# get the predicted values\ny_pred = model.predict(x)\n\n# create the confusion matrix\ncm = confusion_matrix(y, y_pred)\n\n# plot the confusion matrix using fig, ax method\nfig, ax = plt.subplots()\n\nsns.heatmap(cm, annot=True, cmap='Blues', ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual')\nax.set_title('Confusion Matrix')\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\n.score() takes the input and output as arguments and returns the ratio of the number of correct predictions to the number of observations.\n\n10.3.1 We can also use the StatsModels packages, which provides some more statistical details\n\n# import packages\nimport statsmodels.api as sm\n\n# create data\nx = np.arange(10).reshape(-1, 1)\ny = np.array([0, 1, 0, 0, 1, 1, 1, 1, 1, 1])\nx = sm.add_constant(x)\n\n# create model\nmodel = sm.Logit(y, x)\n\n# fit model\nresult = model.fit()\n\n# get results\nresult.params\n\nOptimization terminated successfully.\n         Current function value: 0.350471\n         Iterations 7\n\n\narray([-1.972805  ,  0.82240094])\n\n\n\nprint(result.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                   10\nModel:                          Logit   Df Residuals:                        8\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 22 Jul 2024   Pseudo R-squ.:                  0.4263\nTime:                        09:07:51   Log-Likelihood:                -3.5047\nconverged:                       True   LL-Null:                       -6.1086\nCovariance Type:            nonrobust   LLR p-value:                   0.02248\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.9728      1.737     -1.136      0.256      -5.377       1.431\nx1             0.8224      0.528      1.557      0.119      -0.213       1.858\n=============================================================================="
  },
  {
    "objectID": "tirgul/tirgul_9/multiple_regression.html#multiple-linear-regression",
    "href": "tirgul/tirgul_9/multiple_regression.html#multiple-linear-regression",
    "title": "11  Multiple linear regression",
    "section": "11.1 Multiple Linear Regression",
    "text": "11.1 Multiple Linear Regression\nMultiple Linear Regression Multiple or multivariate linear regression is a case of linear regression with two or more independent variables.\nIf there are just two independent variables, the estimated regression function is 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂. It represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights 𝑏₀, 𝑏₁, and 𝑏₂ such that this plane is as close as possible to the actual responses and yield the minimal SSR.\nThe case of more than two independent variables is similar, but more general. The estimated regression function is 𝑓(𝑥₁, …, 𝑥ᵣ) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ +𝑏ᵣ𝑥ᵣ, and there are 𝑟 + 1 weights to be determined when the number of inputs is 𝑟.\nhttps://realpython.com/linear-regression-in-python/#multiple-linear-regression\nhttps://datatofish.com/multiple-linear-regression-python/\nRequired packages\n\nimport pandas as pd\nfrom sklearn import linear_model\nimport statsmodels.api as sm\nimport seaborn as sns\n\nWe will use the birthweight dataset again\n\n# import dataset\nfile = \"./birthweight.csv\"\ndf = pd.read_csv(file)\ndf.head()\n\n\n\n\n\n\n\n\nLength\nBirthweight\nHeadcirc\nGestation\nsmoker\nmage\nmnocig\nmheight\nmppwt\nfage\nfedyrs\nfnocig\nfheight\nlowbwt\nmage35\n\n\n\n\n0\n56\n4.55\n34\n44\n0\n20\n0\n162\n57\n23\n10\n35\n179\n0\n0\n\n\n1\n53\n4.32\n36\n40\n0\n19\n0\n171\n62\n19\n12\n0\n183\n0\n0\n\n\n2\n58\n4.10\n39\n41\n0\n35\n0\n172\n58\n31\n16\n25\n185\n0\n1\n\n\n3\n53\n4.07\n38\n44\n0\n20\n0\n174\n68\n26\n14\n25\n189\n0\n0\n\n\n4\n54\n3.94\n37\n42\n0\n24\n0\n175\n66\n30\n12\n0\n184\n0\n0\n\n\n\n\n\n\n\n\n11.1.1 Refresher\nLast time we examined the relationship between head circumfrence and birthweight\n\nsns.scatterplot(x=df.Birthweight, y=df.Headcirc)\n\n&lt;Axes: xlabel='Birthweight', ylabel='Headcirc'&gt;\n\n\n\n\n\n\nX = sm.add_constant(df.Birthweight)\nY = df.Headcirc\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Headcirc   R-squared:                       0.469\nModel:                            OLS   Adj. R-squared:                  0.455\nMethod:                 Least Squares   F-statistic:                     35.29\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           5.73e-07\nTime:                        09:09:45   Log-Likelihood:                -82.574\nNo. Observations:                  42   AIC:                             169.1\nDf Residuals:                      40   BIC:                             172.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          25.5824      1.542     16.594      0.000      22.467      28.698\nBirthweight     2.7206      0.458      5.940      0.000       1.795       3.646\n==============================================================================\nOmnibus:                        1.089   Durbin-Watson:                   2.132\nProb(Omnibus):                  0.580   Jarque-Bera (JB):                1.007\nSkew:                          -0.193   Prob(JB):                        0.604\nKurtosis:                       2.347   Cond. No.                         20.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "tirgul/tirgul_9/multiple_regression.html#now-lets-add-another-predicting-variable",
    "href": "tirgul/tirgul_9/multiple_regression.html#now-lets-add-another-predicting-variable",
    "title": "11  Multiple linear regression",
    "section": "11.2 Now, let’s add another predicting variable",
    "text": "11.2 Now, let’s add another predicting variable\nWhat’s the relationship between length and head circumfrence?\n\nsns.scatterplot(x=df.Length, y=df.Headcirc)\n\n&lt;Axes: xlabel='Length', ylabel='Headcirc'&gt;\n\n\n\n\n\nNow we can combine our two ‘predictor’ variables\n\n# group x variables\nX = df[['Birthweight', 'Length']]\n\n# y variable\nY = df['Headcirc']\n\n\n# using statsmodels\nX = sm.add_constant(X)\nmodel = sm.OLS(Y, X).fit()\npredictions = model.predict(X) \nprint_model = model.summary()\nprint(print_model)\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Headcirc   R-squared:                       0.478\nModel:                            OLS   Adj. R-squared:                  0.451\nMethod:                 Least Squares   F-statistic:                     17.84\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           3.14e-06\nTime:                        09:10:16   Log-Likelihood:                -82.211\nNo. Observations:                  42   AIC:                             170.4\nDf Residuals:                      39   BIC:                             175.6\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst          21.0794      5.673      3.716      0.001       9.605      32.554\nBirthweight     2.3191      0.670      3.464      0.001       0.965       3.673\nLength          0.1136      0.138      0.825      0.414      -0.165       0.392\n==============================================================================\nOmnibus:                        1.252   Durbin-Watson:                   2.115\nProb(Omnibus):                  0.535   Jarque-Bera (JB):                1.122\nSkew:                          -0.224   Prob(JB):                        0.571\nKurtosis:                       2.337   Cond. No.                     1.07e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.07e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# with sklearn\nregr = linear_model.LinearRegression()\nregr.fit(X, Y)\n\nprint('Intercept: ', regr.intercept_)\nprint('Coefficients: ', regr.coef_)\n\nIntercept:  21.079365879118836\nCoefficients:  [0.         2.319074   0.11363204]\n\n\nSo, which package should we use?\nIn general, scikit-learn is designed for machine-learning, while statsmodels is made for rigorous statistics.\nhttps://medium.com/(hsrinivasan2/linear-regression-in-scikit-learn-vs-statsmodels-568b60792991?)"
  },
  {
    "objectID": "tirgul/tirgul_9/regularized_regression.html#why-is-it-important",
    "href": "tirgul/tirgul_9/regularized_regression.html#why-is-it-important",
    "title": "12  Regularization",
    "section": "12.1 Why is it important?",
    "text": "12.1 Why is it important?\n\n%matplotlib inline\nfrom IPython.display import Image\nImage('image_1.png')\n\n\n\n\n\n12.1.0.1 In simple terms\nA technique that is used to limit model overfitting by shrinking the coefficient estimates towards zero. Our goal is optimized prediction and not inference.\n\n\n12.1.0.2 In more exact terms\nRegularized least squares (RLS) is a family of methods for solving the least-squares problem while using regularization to further constrain the resulting solution.\nRLS is used for two main reasons. The first comes up when the number of variables in the linear system exceeds the number of observations. In such settings, the ordinary least-squares problem is ill-posed and is therefore impossible to fit because the associated optimization problem has infinitely many solutions. RLS allows the introduction of further constraints that uniquely determine the solution.\nThe second reason that RLS is used occurs when the number of variables does not exceed the number of observations, but the learned model suffers from poor generalization. RLS can be used in such cases to improve the generalizability of the model by constraining it at training time. This constraint can either force the solution to be “sparse” in some way or to reflect other prior knowledge about the problem such as information about correlations between features. A Bayesian understanding of this can be reached by showing that RLS methods are often equivalent to priors on the solution to the least-squares problem.\nhttps://en.wikipedia.org/wiki/Regularized_least_squares\nhttps://scikit-learn.org/stable/modules/linear_model.html\n\n\n12.1.1 We will focus on two methods of regularized regression\n\n12.1.1.1 (1) Ridge\n\nPerforms L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\nMinimization objective = LS Obj + α * (sum of square of coefficients)\n\nhttps://en.wikipedia.org/wiki/Ridge_regression\n\n\n12.1.1.2 (2) Lasso\nhttps://en.wikipedia.org/wiki/Lasso_(statistics)\n\nPerforms L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\nMinimization objective = LS Obj + α * (sum of absolute value of coefficients)\n\n\n\n\n12.1.2 Some notes on usage\nFor both ridge and lasso you have to set a so-called “meta-parameter” that defines how aggressive regularization is performed. Meta-parameters are usually chosen by cross-validation. For Ridge regression the meta-parameter is often called “alpha” or “L2”; it simply defines regularization strength. For LASSO the meta-parameter is often called “lambda”, or “L1”. In contrast to Ridge, the LASSO regularization will actually set less-important predictors to 0 and help you with choosing the predictors that can be left out of the model.\n\n\n12.1.3 What happens when we use more and more predictors?\n\nImage('image_2.png')"
  },
  {
    "objectID": "tirgul/tirgul_9/regularized_regression.html#example",
    "href": "tirgul/tirgul_9/regularized_regression.html#example",
    "title": "12  Regularization",
    "section": "12.2 Example",
    "text": "12.2 Example\nWe will use data from the boston house prediction dataset.\nIn this dataset, each row describes a Boston town or suburb.\nThere are 506 rows and 13 attributes (features) with a target column (price).\n\n# import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n# # load dataset\n# boston_dataset = datasets.fetch_california_housing()\n# # boston_dataset = datasets.load_diabetes()\n# print(boston_dataset.DESCR)\n\n# import dataset\nfile = \"./BostonHousing.csv\"\ndata = pd.read_csv(file)\ndata.head()\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nPRICE\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\n# Check for missing values\ndata.isnull().sum()\n\ncrim       0\nzn         0\nindus      0\nchas       0\nnox        0\nrm         0\nage        0\ndis        0\nrad        0\ntax        0\nptratio    0\nb          0\nlstat      0\nPRICE      0\ndtype: int64\n\n\n\n# take a look at the data\nsns.pairplot(data, corner = \"True\")\n\n\n\n\n\nfig = plt.figure(figsize = (18, 18))\n \nfor ii, feature_name in enumerate(data.columns[:-1]):\n    ax = fig.add_subplot(4, 4, ii + 1)\n    ax.scatter(data.iloc[:, ii], data.iloc[:,-1])\n    ax.set_ylabel('House Price', size = 12)\n    ax.set_xlabel(feature_name, size = 12)\n\n\n\n\n\n12.2.0.1 We can observe from the above scatter plots that some of the independent variables are highly correlated (either positively or negatively) with the target variable. What will happen to these variables when we perform the regularization?"
  },
  {
    "objectID": "tirgul/tirgul_9/regularized_regression.html#start-the-regression",
    "href": "tirgul/tirgul_9/regularized_regression.html#start-the-regression",
    "title": "12  Regularization",
    "section": "12.3 Start the regression :)",
    "text": "12.3 Start the regression :)\n\n12.3.1 Super important!!! standardize the data before applying regularization\n\n# standardize the data by computing the z score \nstand_data = data.apply(lambda x:(x-x.mean())/x.std(), axis=0)\n\n# input\nX = stand_data.iloc[:, :-1]\n \n#output\nY = stand_data.iloc[:, -1]\n\n\n12.3.1.1 Split the data into test and training groups\n\n# split the data into training and testing groups\n\n(x_train, \n x_test, \n y_train, \n y_test) = train_test_split(X, Y, test_size = 0.25)\n \nprint(\"Train data shape of X = % s and Y = % s : \"%(\n    x_train.shape, y_train.shape))\n \nprint(\"Test data shape of X = % s and Y = % s : \"%(\n    x_test.shape, y_test.shape))\n\nTrain data shape of X = (379, 13) and Y = (379,) : \nTest data shape of X = (127, 13) and Y = (127,) :"
  },
  {
    "objectID": "tirgul/tirgul_9/regularized_regression.html#multiple-linear-regression",
    "href": "tirgul/tirgul_9/regularized_regression.html#multiple-linear-regression",
    "title": "12  Regularization",
    "section": "12.4 Multiple Linear Regression",
    "text": "12.4 Multiple Linear Regression\nFirst, let’s try multiple linear regression, so that we have a point of comparison\n\n# Apply multiple Linear Regression Model\nlreg = LinearRegression()\nlreg.fit(x_train, y_train)\n \n# Generate Prediction on test set\nlreg_y_pred = lreg.predict(x_test)\n \n# calculating Mean Squared Error (mse)\nmean_squared_error = np.mean((lreg_y_pred - y_test)**2)\nprint(\"Mean squared Error on test set : \", mean_squared_error)\n \n# Putting together the coefficients and their corresponding variable names\nlreg_coefficient = pd.DataFrame()\nlreg_coefficient[\"Columns\"] = x_train.columns\nlreg_coefficient['Coefficient Estimate'] = pd.Series(lreg.coef_)\nlreg_coefficient['Type'] = 'Linear (MSE: '+ str(np.around(mean_squared_error,2))+')'\n\nMean squared Error on test set :  0.16355502227534496\n\n\n\n# plotting the coefficient score\nfig, ax = plt.subplots(figsize =(10, 5))\n\nsns.barplot(x = lreg_coefficient[\"Columns\"],\n            y = lreg_coefficient['Coefficient Estimate'],\n            hue = None) \n\n&lt;Axes: xlabel='Columns', ylabel='Coefficient Estimate'&gt;\n\n\n\n\n\n\n12.4.1 Now, let’s try Ridge regression\n\n# Train the model\nalpha = 1\nridgeR = Ridge(alpha = alpha)\nridgeR.fit(x_train, y_train)\ny_pred = ridgeR.predict(x_test)\n \n# calculate mean square error\nmean_squared_error_ridge = np.mean((y_pred - y_test)**2)\nprint(\"Mean squared Error on test set : \", mean_squared_error_ridge)\n \n# get ridge coefficient\nridge_coefficient = pd.DataFrame()\nridge_coefficient[\"Columns\"]= x_train.columns\nridge_coefficient['Coefficient Estimate'] = pd.Series(ridgeR.coef_)\nridge_coefficient['Type'] = r'Ridge, $\\lambda$ = '+ f'{alpha} (MSE: {str(np.around(mean_squared_error,2))})'\n\nMean squared Error on test set :  0.16328472190858787\n\n\n\n# merge dataframes\nframes = [lreg_coefficient,\n         ridge_coefficient]\n\nall_coefs = pd.concat(frames)\n\n# plotting the coefficient scores\nfig, ax = plt.subplots(figsize =(20, 10))\n \nsns.barplot(x = all_coefs[\"Columns\"],\n            y = all_coefs['Coefficient Estimate'],\n            hue = all_coefs['Type']) \n\n&lt;Axes: xlabel='Columns', ylabel='Coefficient Estimate'&gt;\n\n\n\n\n\n\n# Train the model\nalpha = 5000\nridgeR = Ridge(alpha = alpha)\nridgeR.fit(x_train, y_train)\ny_pred = ridgeR.predict(x_test)\n \n# calculate mean square error\nmean_squared_error_ridge = np.mean((y_pred - y_test)**2)\n \n# get ridge coefficient\nridge_coefficient_10 = pd.DataFrame()\nridge_coefficient_10[\"Columns\"]= x_train.columns\nridge_coefficient_10['Coefficient Estimate'] = pd.Series(ridgeR.coef_)\n# r'Ridge, $\\lambda$ = '+ f'{alpha} (MSE: {str(np.around(mean_squared_error,2))})'\nridge_coefficient_10['Type'] = r'Ridge, $\\lambda$ = '+ f'{alpha} (MSE: {str(np.around(mean_squared_error,2))})'\n\n# merge dataframes\nframes = [lreg_coefficient,\n         ridge_coefficient,\n         ridge_coefficient_10]\n\nall_coefs = pd.concat(frames)\n\n# plotting the coefficient scores\nfig, ax = plt.subplots(figsize =(20, 10))\n \nsns.barplot(x = all_coefs[\"Columns\"],\n            y = all_coefs['Coefficient Estimate'],\n            hue = all_coefs['Type']) \n\n&lt;Axes: xlabel='Columns', ylabel='Coefficient Estimate'&gt;\n\n\n\n\n\n\n\n12.4.2 Now, let’s try Lasso\n\n# Train the model\nalpha = 0.01\nlasso = Lasso(alpha = alpha)\nlasso.fit(x_train, y_train)\ny_pred1 = lasso.predict(x_test)\n \n# Calculate Mean Squared Error\nmean_squared_error = np.mean((y_pred1 - y_test)**2)\n\n# Put in dataframe\nlasso_coeff = pd.DataFrame()\nlasso_coeff[\"Columns\"] = x_train.columns\nlasso_coeff['Coefficient Estimate'] = pd.Series(lasso.coef_)\nlasso_coeff['Type'] = r'Lasso, $\\lambda$ = '+ f'{alpha} (MSE: {str(np.around(mean_squared_error,2))})'\n\n# merge dataframes\nframes = [lreg_coefficient,\n         ridge_coefficient,\n         ridge_coefficient_10,\n         lasso_coeff]\n\nall_coefs = pd.concat(frames)\n\n# plotting the coefficient scores\nfig, ax = plt.subplots(figsize =(20, 10))\n \nsns.barplot(x = all_coefs[\"Columns\"],\n            y = all_coefs['Coefficient Estimate'],\n            hue = all_coefs['Type']) \n\n&lt;Axes: xlabel='Columns', ylabel='Coefficient Estimate'&gt;\n\n\n\n\n\n\n# Train the model\nalpha = 0.1\nlasso = Lasso(alpha = alpha)\nlasso.fit(x_train, y_train)\ny_pred1 = lasso.predict(x_test)\n \n# Calculate Mean Squared Error\nmean_squared_error = np.mean((y_pred1 - y_test)**2)\n\n# Put in dataframe\nlasso_coeff_10 = pd.DataFrame()\nlasso_coeff_10[\"Columns\"] = x_train.columns\nlasso_coeff_10['Coefficient Estimate'] = pd.Series(lasso.coef_)\nlasso_coeff_10['Type'] = r'Lasso, $\\lambda$ = '+ f'{alpha} (MSE: {str(np.around(mean_squared_error,2))})'\n\n# merge dataframes\nframes = [lreg_coefficient,\n         ridge_coefficient,\n         ridge_coefficient_10,\n         lasso_coeff,\n         lasso_coeff_10]\n\nall_coefs = pd.concat(frames)\n\n# plotting the coefficient scores\nfig, ax = plt.subplots(figsize =(20, 10))\n \nsns.barplot(x = all_coefs[\"Columns\"],\n            y = all_coefs['Coefficient Estimate'],\n            hue = all_coefs['Type']) \n\n&lt;Axes: xlabel='Columns', ylabel='Coefficient Estimate'&gt;"
  }
]